{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing adi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import re\n",
    "import glob\n",
    "from utils.eval_utils import class_attribution_metric, attribution_metric\n",
    "from utils import RIVAL10_constants\n",
    "\n",
    "spss = r\"^spss_vl_cbm_train_simple_concepts_([\\d.]+)_([\\d.]+)_([\\d.]+)$\"\n",
    "cspss = r\"^cspss_vl_cbm_train_simple_concepts_([\\d.]+)_([\\d.]+)_([\\d.]+)_([\\d.]+)$\"\n",
    "# spss_softmax = r\"^spss_vl_cbm_train_simple_concepts_([\\d.]+)_([\\d.]+)_([\\d.]+)\"\n",
    "# cspss_softmax = (\n",
    "#     r\"^cspss_vl_cbm_train_simple_concepts_([\\d.]+)_([\\d.]+)_([\\d.]+)_([\\d.]+)\"\n",
    "# )\n",
    "\n",
    "\n",
    "def collect_pt_files(folder_path, regax_str: str):\n",
    "    \"\"\"\n",
    "    收集指定文件夹下所有符合条件的子文件夹中的 .pt 文件，并按三个浮点数组织成字典。\n",
    "\n",
    "    参数:\n",
    "        folder_path (str): 根文件夹路径。\n",
    "\n",
    "    返回:\n",
    "        dict: 格式为 {(float, float, float): {pt_file_name: pt_file_path, ...}, ...}\n",
    "    \"\"\"\n",
    "    # 使用 glob 匹配所有符合条件的子文件夹\n",
    "    pattern = os.path.join(folder_path, \"*\")\n",
    "    matched_folders = glob.glob(pattern)\n",
    "\n",
    "    # 定义正则表达式提取三个浮点数\n",
    "    regex = re.compile(regax_str)\n",
    "\n",
    "    # 创建结果字典\n",
    "    result_dict = {}\n",
    "\n",
    "    # 遍历匹配的文件夹\n",
    "    for folder in matched_folders:\n",
    "        # 提取三个浮点数\n",
    "        match = regex.search(os.path.basename(folder))\n",
    "        if match:\n",
    "            floats = tuple(map(float, match.groups()))  # 转换为浮点数元组\n",
    "\n",
    "            # 构建 evaluations 文件夹路径\n",
    "            print(os.path.basename(folder))\n",
    "            evaluations_path = os.path.join(folder, \"evaluations\")\n",
    "\n",
    "            # 检查 evaluations 文件夹是否存在\n",
    "            if os.path.exists(evaluations_path) and os.path.isdir(evaluations_path):\n",
    "                # 获取 evaluations 文件夹下的所有 .pt 文件\n",
    "                pt_files = glob.glob(os.path.join(evaluations_path, \"*.pt\"))\n",
    "\n",
    "                # 将 .pt 文件以文件名（不含扩展名）为键，存储到字典中\n",
    "                pt_dict = {}\n",
    "                for pt_file in pt_files:\n",
    "                    file_name = os.path.splitext(os.path.basename(pt_file))[\n",
    "                        0\n",
    "                    ]  # 去掉扩展名\n",
    "                    pt_dict[file_name] = pt_file  # 存储文件路径\n",
    "\n",
    "                # 将结果存储到主字典中\n",
    "                result_dict[floats] = pt_dict\n",
    "\n",
    "    sorted_result_dict = dict(sorted(result_dict.items()))\n",
    "\n",
    "    return sorted_result_dict\n",
    "\n",
    "\n",
    "def collect_acc_files(folder_path, regax_str: str):\n",
    "    \"\"\"\n",
    "    收集指定文件夹下所有符合条件的子文件夹中的 .pt 文件，并按三个浮点数组织成字典。\n",
    "\n",
    "    参数:\n",
    "        folder_path (str): 根文件夹路径。\n",
    "\n",
    "    返回:\n",
    "        dict: 格式为 {(float, float, float): exp_log.log\n",
    "    \"\"\"\n",
    "    # 使用 glob 匹配所有符合条件的子文件夹\n",
    "    pattern = os.path.join(folder_path, \"*\")\n",
    "    matched_folders = glob.glob(pattern)\n",
    "\n",
    "    # 定义正则表达式提取三个浮点数\n",
    "    regex = re.compile(regax_str)\n",
    "\n",
    "    # 创建结果字典\n",
    "    result_dict = {}\n",
    "\n",
    "    # 遍历匹配的文件夹\n",
    "    for folder in matched_folders:\n",
    "        # 提取三个浮点数\n",
    "        match = regex.search(os.path.basename(folder))\n",
    "        if match:\n",
    "            floats = tuple(map(float, match.groups()))  # 转换为浮点数元组\n",
    "\n",
    "            # 构建 evaluations 文件夹路径\n",
    "            print(os.path.basename(folder))\n",
    "            log_path = os.path.join(folder, \"exp_log.log\")\n",
    "\n",
    "            # 检查 evaluations 文件夹是否存在\n",
    "            if os.path.exists(log_path) and not os.path.isdir(log_path):\n",
    "                result_dict[floats] = log_path\n",
    "\n",
    "    sorted_result_dict = dict(sorted(result_dict.items()))\n",
    "\n",
    "    return sorted_result_dict\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_last_accuracies(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # 正则表达式匹配 \"Val Class Accuracy = xxx.xx\" 和 \"Val Concept Accuracy = xxx.xx\"\n",
    "    pattern = r\"Val Class Accuracy = (\\d+\\.\\d+).*?Val Concept Accuracy = (\\d+\\.\\d+)\"\n",
    "\n",
    "    matches = re.findall(pattern, text)\n",
    "\n",
    "    if matches:\n",
    "        # 取最后一个匹配的结果\n",
    "        last_class_acc, last_concept_acc = matches[-1]\n",
    "        return float(last_class_acc), float(last_concept_acc)\n",
    "    else:\n",
    "        return None  # 没有找到匹配项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spss_vl_cbm_train_simple_concepts_1.0_1.0_5.0\n",
      "spss_vl_cbm_train_simple_concepts_1.0_1.0_1.0\n",
      "spss_vl_cbm_train_simple_concepts_1.0_1.0_10.0\n",
      "spss_vl_cbm_train_simple_concepts_1.0_1.0_0.0\n",
      "spss_vl_cbm_train_simple_concepts_1.0_1.0_1.5\n",
      "spss_vl_cbm_train_simple_concepts_1.0_1.0_0.1\n",
      "spss_vl_cbm_train_simple_concepts_1.0_1.0_2.0\n",
      "spss_vl_cbm_train_simple_concepts_1.0_1.0_0.5\n",
      "1.0\t1e4\t0.0\t0.0\t98.28\t89.93\t\t24.43\t\t33.51\t\t32.72\t\t1.27\t48.17\t4.00\n",
      "1.0\t1e4\t0.1\t0.0\t98.35\t89.79\t\t22.75\t\t31.45\t\t31.84\t\t1.33\t47.65\t4.25\n",
      "1.0\t1e4\t0.5\t0.0\t98.51\t89.57\t\t20.33\t\t28.96\t\t33.07\t\t1.68\t45.82\t5.26\n",
      "1.0\t1e4\t1.0\t0.0\t98.60\t89.27\t\t19.53\t\t28.20\t\t35.08\t\t1.83\t45.33\t5.62\n",
      "1.0\t1e4\t1.5\t0.0\t98.60\t88.55\t\t19.26\t\t28.15\t\t37.60\t\t1.96\t44.98\t5.71\n",
      "1.0\t1e4\t2.0\t0.0\t98.56\t88.01\t\t18.29\t\t27.07\t\t37.67\t\t1.96\t44.55\t5.59\n",
      "1.0\t1e4\t5.0\t0.0\t98.73\t84.44\t\t17.14\t\t25.48\t\t38.44\t\t1.76\t44.08\t4.39\n",
      "1.0\t1e4\t10.0\t0.0\t89.84\t83.28\t\t15.70\t\t23.69\t\t42.56\t\t1.77\t42.62\t3.00\n",
      "cspss_vl_cbm_train_simple_concepts_1.0_1.0_10.0_1.0\n",
      "cspss_vl_cbm_train_simple_concepts_1.0_1.0_0.1_1.0\n",
      "cspss_vl_cbm_train_simple_concepts_1.0_1.0_1.0_1.0\n",
      "cspss_vl_cbm_train_simple_concepts_1.0_1.0_2.0_1.0\n",
      "cspss_vl_cbm_train_simple_concepts_1.0_1.0_1.5_1.0\n",
      "cspss_vl_cbm_train_simple_concepts_1.0_1.0_0.0_1.0\n",
      "cspss_vl_cbm_train_simple_concepts_1.0_1.0_5.0_1.0\n",
      "cspss_vl_cbm_train_simple_concepts_1.0_1.0_0.5_1.0\n",
      "dict_keys(['classes_segmentation_metric', 'concepts_segmentation_metric', 'weighted_cocnepts_metric', 'nec', 'accuracy', 'adi_pack'])\n",
      "dict_keys(['classes_segmentation_metric', 'concepts_segmentation_metric', 'weighted_cocnepts_metric', 'nec', 'accuracy', 'adi_pack'])\n",
      "dict_keys(['classes_segmentation_metric', 'concepts_segmentation_metric', 'weighted_cocnepts_metric', 'nec', 'accuracy', 'adi_pack'])\n",
      "dict_keys(['classes_segmentation_metric', 'concepts_segmentation_metric', 'weighted_cocnepts_metric', 'nec', 'accuracy', 'adi_pack'])\n",
      "dict_keys(['classes_segmentation_metric', 'concepts_segmentation_metric', 'weighted_cocnepts_metric', 'nec', 'accuracy', 'adi_pack'])\n",
      "dict_keys(['classes_segmentation_metric', 'concepts_segmentation_metric', 'weighted_cocnepts_metric', 'nec', 'accuracy', 'adi_pack'])\n",
      "dict_keys(['classes_segmentation_metric', 'concepts_segmentation_metric', 'weighted_cocnepts_metric', 'nec', 'accuracy', 'adi_pack'])\n",
      "dict_keys(['classes_segmentation_metric', 'concepts_segmentation_metric', 'weighted_cocnepts_metric', 'nec', 'accuracy', 'adi_pack'])\n",
      "1.0\t1e4\t0.0\t0.0\t98.28\t89.93\t\t24.43\t\t33.51\t\t32.72\t\t1.27\t48.17\t4.00\n",
      "1.0\t1e4\t0.1\t0.0\t98.35\t89.79\t\t22.75\t\t31.45\t\t31.84\t\t1.33\t47.65\t4.25\n",
      "1.0\t1e4\t0.5\t0.0\t98.51\t89.57\t\t20.33\t\t28.96\t\t33.07\t\t1.68\t45.82\t5.26\n",
      "1.0\t1e4\t1.0\t0.0\t98.60\t89.27\t\t19.53\t\t28.20\t\t35.08\t\t1.83\t45.33\t5.62\n",
      "1.0\t1e4\t1.5\t0.0\t98.60\t88.55\t\t19.26\t\t28.15\t\t37.60\t\t1.96\t44.98\t5.71\n",
      "1.0\t1e4\t2.0\t0.0\t98.56\t88.01\t\t18.29\t\t27.07\t\t37.67\t\t1.96\t44.55\t5.59\n",
      "1.0\t1e4\t5.0\t0.0\t98.73\t84.44\t\t17.14\t\t25.48\t\t38.44\t\t1.76\t44.08\t4.39\n",
      "1.0\t1e4\t10.0\t0.0\t89.84\t83.28\t\t15.70\t\t23.69\t\t42.56\t\t1.77\t42.62\t3.00\n",
      "1.0\t1e4\t0.0\t1.0\t69.39\t90.16\t\t21.78\t\t30.63\t\t30.28\t\t1.05\t47.12\t3.20\n",
      "1.0\t1e4\t0.1\t1.0\t69.64\t90.14\t\t20.53\t\t28.95\t\t29.32\t\t1.05\t47.24\t3.23\n",
      "1.0\t1e4\t0.5\t1.0\t70.58\t89.95\t\t19.57\t\t27.50\t\t30.27\t\t1.21\t47.46\t3.79\n",
      "1.0\t1e4\t1.0\t1.0\t70.75\t89.67\t\t17.55\t\t25.37\t\t28.46\t\t1.31\t46.70\t4.17\n",
      "1.0\t1e4\t1.5\t1.0\t72.34\t88.34\t\t17.49\t\t25.42\t\t29.78\t\t1.56\t44.09\t4.30\n",
      "1.0\t1e4\t2.0\t1.0\t71.21\t86.02\t\t17.06\t\t24.97\t\t28.75\t\t1.47\t43.73\t3.84\n",
      "1.0\t1e4\t5.0\t1.0\t66.95\t83.79\t\t16.57\t\t24.05\t\t27.76\t\t1.33\t41.48\t2.66\n",
      "1.0\t1e4\t10.0\t1.0\t53.25\t84.11\t\t16.62\t\t23.84\t\t28.17\t\t1.41\t40.89\t2.34\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "folder_path = \"../outputs/eval/lambda_ablation_1e-1\"\n",
    "result = collect_pt_files(folder_path, spss)\n",
    "\n",
    "adi_txt = []\n",
    "\n",
    "for floats, pt_dict in result.items():\n",
    "    # print(f\"Floats: {floats}\")\n",
    "    adi_txt.append(f\"1.0\\t1e4\\t{floats[2]}\\t0.0\\t\")\n",
    "    for file_name in [\"accuracy\", \"concepts_segmentation_metric\", \"adi_pack\"]:\n",
    "        if file_name.startswith(\"classes\"):\n",
    "            metric = class_attribution_metric(\"\", 0)\n",
    "            metric.load_from_path(pt_dict[file_name])\n",
    "            # print(metric.format_output(RIVAL10_constants._ALL_CLASSNAMES, latex=True))\n",
    "        elif file_name == \"accuracy\":\n",
    "            metric = torch.load(pt_dict[file_name])\n",
    "            adi_txt[-1] += (f\"{metric[\"test_acc\"]:.2f}\\t{metric[\"test_concept_acc\"]:.2f}\\t\"\n",
    "            )\n",
    "        elif file_name == \"adi_pack\":\n",
    "            metric = torch.load(pt_dict[file_name])\n",
    "            adi_txt[-1] += (f\"\\t{metric[\"concepts_avg_drop\"].item() * 100:.2f}\\t{metric[\"concepts_avg_inc\"].item() * 100:.2f}\\t{metric[\"concepts_avg_gain\"].item() * 100:.2f}\")\n",
    "\n",
    "        elif file_name == \"concepts_segmentation_metric\":\n",
    "            metric = attribution_metric(\"\", 0, 0)\n",
    "            metric.load_from_path(pt_dict[file_name])\n",
    "            # print(f\"1.0\\t1e4\\t{floats[2]}\\t0.0\\t{adi_pack[\"avg_drop\"].item():.2f}\\t{adi_pack[\"avg_gain\"]:.2f}\\t{adi_pack[\"avg_inc\"]:.2f}\")\n",
    "            adi_txt[-1] += (metric.format_output(\n",
    "                    RIVAL10_constants._ALL_CLASSNAMES,\n",
    "                    RIVAL10_constants._ALL_ATTRS,\n",
    "                    latex=True,\n",
    "                    sep=\"\\t\"\n",
    "                )\n",
    "            )\n",
    "print(\"\\n\".join(adi_txt))\n",
    "\n",
    "folder_path = \"../outputs/eval/lambda_ablation_1e-1\"\n",
    "result = collect_pt_files(folder_path, cspss)\n",
    "\n",
    "for floats, pt_dict in result.items():\n",
    "    # print(f\"Floats: {floats}\")\n",
    "    print(pt_dict.keys())\n",
    "    adi_txt.append(f\"1.0\\t1e4\\t{floats[2]}\\t1.0\\t\")\n",
    "    for file_name in [\"accuracy\", \"concepts_segmentation_metric\", \"adi_pack\"]:\n",
    "        if file_name.startswith(\"classes\"):\n",
    "            metric = class_attribution_metric(\"\", 0)\n",
    "            metric.load_from_path(pt_dict[file_name])\n",
    "            # print(metric.format_output(RIVAL10_constants._ALL_CLASSNAMES, latex=True))\n",
    "        elif file_name == \"accuracy\":\n",
    "            metric = torch.load(pt_dict[file_name])\n",
    "            adi_txt[-1] += (f\"{metric[\"test_acc\"]:.2f}\\t{metric[\"test_concept_acc\"]:.2f}\\t\"\n",
    "            )\n",
    "        elif file_name == \"adi_pack\":\n",
    "            metric = torch.load(pt_dict[file_name])\n",
    "            adi_txt[-1] += (f\"\\t{metric[\"concepts_avg_drop\"].item() * 100:.2f}\\t{metric[\"concepts_avg_inc\"].item() * 100:.2f}\\t{metric[\"concepts_avg_gain\"].item() * 100:.2f}\")\n",
    "\n",
    "        elif file_name == \"concepts_segmentation_metric\":\n",
    "            metric = attribution_metric(\"\", 0, 0)\n",
    "            metric.load_from_path(pt_dict[file_name])\n",
    "            # print(f\"1.0\\t1e4\\t{floats[2]}\\t0.0\\t{adi_pack[\"avg_drop\"].item():.2f}\\t{adi_pack[\"avg_gain\"]:.2f}\\t{adi_pack[\"avg_inc\"]:.2f}\")\n",
    "            adi_txt[-1] += (metric.format_output(\n",
    "                    RIVAL10_constants._ALL_CLASSNAMES,\n",
    "                    RIVAL10_constants._ALL_ATTRS,\n",
    "                    latex=True,\n",
    "                    sep=\"\\t\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "print(\"\\n\".join(adi_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spss_vl_cbm_train_simple_concepts_1.0_1.0_5.0\n",
      "spss_vl_cbm_train_simple_concepts_1.0_1.0_1.0\n",
      "spss_vl_cbm_train_simple_concepts_1.0_1.0_10.0\n",
      "spss_vl_cbm_train_simple_concepts_1.0_1.0_0.0\n",
      "spss_vl_cbm_train_simple_concepts_1.0_1.0_1.5\n",
      "spss_vl_cbm_train_simple_concepts_1.0_1.0_0.1\n",
      "spss_vl_cbm_train_simple_concepts_1.0_1.0_2.0\n",
      "spss_vl_cbm_train_simple_concepts_1.0_1.0_0.5\n",
      "1.0\t1e4\t0.0\t0.0\t99.17\t90.37\t\t22.95\t\t32.02\t\t33.89\t\t2.39\t50.44\t8.75\n",
      "1.0\t1e4\t0.1\t0.0\t99.28\t90.42\t\t24.68\t\t33.22\t\t37.87\t\t2.38\t49.47\t8.91\n",
      "1.0\t1e4\t0.5\t0.0\t99.09\t90.15\t\t20.53\t\t29.07\t\t34.72\t\t2.63\t48.15\t9.86\n",
      "1.0\t1e4\t1.0\t0.0\t98.90\t90.07\t\t16.24\t\t23.76\t\t34.54\t\t2.81\t47.87\t10.02\n",
      "1.0\t1e4\t1.5\t0.0\t98.79\t89.91\t\t15.88\t\t23.56\t\t35.42\t\t2.80\t46.98\t9.81\n",
      "1.0\t1e4\t2.0\t0.0\t98.79\t89.68\t\t15.12\t\t22.67\t\t35.53\t\t2.80\t46.76\t9.65\n",
      "1.0\t1e4\t5.0\t0.0\t98.85\t89.65\t\t15.63\t\t23.41\t\t38.98\t\t2.65\t46.07\t8.20\n",
      "1.0\t1e4\t10.0\t0.0\t98.96\t87.34\t\t10.72\t\t16.87\t\t39.05\t\t2.80\t42.63\t6.13\n",
      "cspss_vl_cbm_train_simple_concepts_1.0_1.0_10.0_1.0\n",
      "cspss_vl_cbm_train_simple_concepts_1.0_1.0_0.1_1.0\n",
      "cspss_vl_cbm_train_simple_concepts_1.0_1.0_1.0_1.0\n",
      "cspss_vl_cbm_train_simple_concepts_1.0_1.0_2.0_1.0\n",
      "cspss_vl_cbm_train_simple_concepts_1.0_1.0_1.5_1.0\n",
      "cspss_vl_cbm_train_simple_concepts_1.0_1.0_0.0_1.0\n",
      "cspss_vl_cbm_train_simple_concepts_1.0_1.0_5.0_1.0\n",
      "cspss_vl_cbm_train_simple_concepts_1.0_1.0_0.5_1.0\n",
      "dict_keys(['classes_segmentation_metric', 'concepts_segmentation_metric', 'weighted_cocnepts_metric', 'nec', 'accuracy', 'adi_pack'])\n",
      "dict_keys(['classes_segmentation_metric', 'concepts_segmentation_metric', 'weighted_cocnepts_metric', 'nec', 'accuracy', 'adi_pack'])\n",
      "dict_keys(['classes_segmentation_metric', 'concepts_segmentation_metric', 'weighted_cocnepts_metric', 'nec', 'accuracy', 'adi_pack'])\n",
      "dict_keys(['classes_segmentation_metric', 'concepts_segmentation_metric', 'weighted_cocnepts_metric', 'nec', 'accuracy', 'adi_pack'])\n",
      "dict_keys(['classes_segmentation_metric', 'concepts_segmentation_metric', 'weighted_cocnepts_metric', 'nec', 'accuracy', 'adi_pack'])\n",
      "dict_keys(['classes_segmentation_metric', 'concepts_segmentation_metric', 'weighted_cocnepts_metric', 'nec', 'accuracy', 'adi_pack'])\n",
      "dict_keys(['classes_segmentation_metric', 'concepts_segmentation_metric', 'weighted_cocnepts_metric', 'nec', 'accuracy', 'adi_pack'])\n",
      "dict_keys(['classes_segmentation_metric', 'concepts_segmentation_metric', 'weighted_cocnepts_metric', 'nec', 'accuracy', 'adi_pack'])\n",
      "1.0\t1e4\t0.0\t0.0\t99.17\t90.37\t\t22.95\t\t32.02\t\t33.89\t\t2.39\t50.44\t8.75\n",
      "1.0\t1e4\t0.1\t0.0\t99.28\t90.42\t\t24.68\t\t33.22\t\t37.87\t\t2.38\t49.47\t8.91\n",
      "1.0\t1e4\t0.5\t0.0\t99.09\t90.15\t\t20.53\t\t29.07\t\t34.72\t\t2.63\t48.15\t9.86\n",
      "1.0\t1e4\t1.0\t0.0\t98.90\t90.07\t\t16.24\t\t23.76\t\t34.54\t\t2.81\t47.87\t10.02\n",
      "1.0\t1e4\t1.5\t0.0\t98.79\t89.91\t\t15.88\t\t23.56\t\t35.42\t\t2.80\t46.98\t9.81\n",
      "1.0\t1e4\t2.0\t0.0\t98.79\t89.68\t\t15.12\t\t22.67\t\t35.53\t\t2.80\t46.76\t9.65\n",
      "1.0\t1e4\t5.0\t0.0\t98.85\t89.65\t\t15.63\t\t23.41\t\t38.98\t\t2.65\t46.07\t8.20\n",
      "1.0\t1e4\t10.0\t0.0\t98.96\t87.34\t\t10.72\t\t16.87\t\t39.05\t\t2.80\t42.63\t6.13\n",
      "1.0\t1e4\t0.0\t1.0\t99.15\t90.65\t\t24.26\t\t33.69\t\t36.69\t\t1.62\t50.35\t5.55\n",
      "1.0\t1e4\t0.1\t1.0\t99.04\t90.59\t\t23.22\t\t32.52\t\t39.11\t\t1.72\t50.23\t5.96\n",
      "1.0\t1e4\t0.5\t1.0\t99.17\t90.16\t\t16.12\t\t24.17\t\t35.43\t\t1.91\t48.30\t6.62\n",
      "1.0\t1e4\t1.0\t1.0\t99.04\t90.04\t\t14.72\t\t22.41\t\t35.71\t\t2.04\t47.10\t6.84\n",
      "1.0\t1e4\t1.5\t1.0\t99.24\t89.82\t\t14.53\t\t22.18\t\t36.08\t\t2.08\t46.10\t6.80\n",
      "1.0\t1e4\t2.0\t1.0\t99.17\t89.75\t\t14.22\t\t21.83\t\t36.60\t\t2.10\t45.83\t6.76\n",
      "1.0\t1e4\t5.0\t1.0\t99.19\t87.93\t\t13.27\t\t20.56\t\t37.93\t\t1.93\t44.33\t5.54\n",
      "1.0\t1e4\t10.0\t1.0\t99.07\t85.12\t\t13.05\t\t20.03\t\t37.46\t\t1.63\t45.23\t4.51\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "folder_path = \"../outputs/eval/lambda_ablation_5e-1\"\n",
    "result = collect_pt_files(folder_path, spss)\n",
    "\n",
    "adi_txt = []\n",
    "\n",
    "for floats, pt_dict in result.items():\n",
    "    # print(f\"Floats: {floats}\")\n",
    "    adi_txt.append(f\"1.0\\t1e4\\t{floats[2]}\\t0.0\\t\")\n",
    "    for file_name in [\"accuracy\", \"concepts_segmentation_metric\", \"adi_pack\"]:\n",
    "        if file_name.startswith(\"classes\"):\n",
    "            metric = class_attribution_metric(\"\", 0)\n",
    "            metric.load_from_path(pt_dict[file_name])\n",
    "            # print(metric.format_output(RIVAL10_constants._ALL_CLASSNAMES, latex=True))\n",
    "        elif file_name == \"accuracy\":\n",
    "            metric = torch.load(pt_dict[file_name])\n",
    "            adi_txt[-1] += (f\"{metric[\"test_acc\"]:.2f}\\t{metric[\"test_concept_acc\"]:.2f}\\t\"\n",
    "            )\n",
    "        elif file_name == \"adi_pack\":\n",
    "            metric = torch.load(pt_dict[file_name])\n",
    "            adi_txt[-1] += (f\"\\t{metric[\"concepts_avg_drop\"].item() * 100:.2f}\\t{metric[\"concepts_avg_inc\"].item() * 100:.2f}\\t{metric[\"concepts_avg_gain\"].item() * 100:.2f}\")\n",
    "\n",
    "        elif file_name == \"concepts_segmentation_metric\":\n",
    "            metric = attribution_metric(\"\", 0, 0)\n",
    "            metric.load_from_path(pt_dict[file_name])\n",
    "            # print(f\"1.0\\t1e4\\t{floats[2]}\\t0.0\\t{adi_pack[\"avg_drop\"].item():.2f}\\t{adi_pack[\"avg_gain\"]:.2f}\\t{adi_pack[\"avg_inc\"]:.2f}\")\n",
    "            adi_txt[-1] += (metric.format_output(\n",
    "                    RIVAL10_constants._ALL_CLASSNAMES,\n",
    "                    RIVAL10_constants._ALL_ATTRS,\n",
    "                    latex=True,\n",
    "                    sep=\"\\t\"\n",
    "                )\n",
    "            )\n",
    "print(\"\\n\".join(adi_txt))\n",
    "\n",
    "folder_path = \"../outputs/eval/lambda_ablation_5e-1\"\n",
    "result = collect_pt_files(folder_path, cspss)\n",
    "\n",
    "for floats, pt_dict in result.items():\n",
    "    # print(f\"Floats: {floats}\")\n",
    "    print(pt_dict.keys())\n",
    "    adi_txt.append(f\"1.0\\t1e4\\t{floats[2]}\\t1.0\\t\")\n",
    "    for file_name in [\"accuracy\", \"concepts_segmentation_metric\", \"adi_pack\"]:\n",
    "        if file_name.startswith(\"classes\"):\n",
    "            metric = class_attribution_metric(\"\", 0)\n",
    "            metric.load_from_path(pt_dict[file_name])\n",
    "            # print(metric.format_output(RIVAL10_constants._ALL_CLASSNAMES, latex=True))\n",
    "        elif file_name == \"accuracy\":\n",
    "            metric = torch.load(pt_dict[file_name])\n",
    "            adi_txt[-1] += (f\"{metric[\"test_acc\"]:.2f}\\t{metric[\"test_concept_acc\"]:.2f}\\t\"\n",
    "            )\n",
    "        elif file_name == \"adi_pack\":\n",
    "            metric = torch.load(pt_dict[file_name])\n",
    "            adi_txt[-1] += (f\"\\t{metric[\"concepts_avg_drop\"].item() * 100:.2f}\\t{metric[\"concepts_avg_inc\"].item() * 100:.2f}\\t{metric[\"concepts_avg_gain\"].item() * 100:.2f}\")\n",
    "\n",
    "        elif file_name == \"concepts_segmentation_metric\":\n",
    "            metric = attribution_metric(\"\", 0, 0)\n",
    "            metric.load_from_path(pt_dict[file_name])\n",
    "            # print(f\"1.0\\t1e4\\t{floats[2]}\\t0.0\\t{adi_pack[\"avg_drop\"].item():.2f}\\t{adi_pack[\"avg_gain\"]:.2f}\\t{adi_pack[\"avg_inc\"]:.2f}\")\n",
    "            adi_txt[-1] += (metric.format_output(\n",
    "                    RIVAL10_constants._ALL_CLASSNAMES,\n",
    "                    RIVAL10_constants._ALL_ATTRS,\n",
    "                    latex=True,\n",
    "                    sep=\"\\t\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "print(\"\\n\".join(adi_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "\n",
    "spss = r\"spss_vl_cbm_train_simple_concepts_([\\d.]+)_([\\d.]+)_([\\d.]+)$\"\n",
    "cspss = r\"^cspss_vl_cbm_train_simple_concepts_([\\d.]+)_([\\d.]+)_([\\d.]+)_([\\d.]+)$\"\n",
    "# spss_softmax = r\"^eval_spss_vl_cbm_train_simple_concepts_([\\d.]+)_([\\d.]+)_([\\d.]+)$\"\n",
    "# cspss_softmax = (\n",
    "#     r\"^eval_cspss_vl_cbm_train_simple_concepts_([\\d.]+)_([\\d.]+)_([\\d.]+)_([\\d.]+)$\"\n",
    "# )\n",
    "\n",
    "\n",
    "def collect_pt_files(folder_path, regax_str: str):\n",
    "    \"\"\"\n",
    "    收集指定文件夹下所有符合条件的子文件夹中的 .pt 文件，并按三个浮点数组织成字典。\n",
    "\n",
    "    参数:\n",
    "        folder_path (str): 根文件夹路径。\n",
    "\n",
    "    返回:\n",
    "        dict: 格式为 {(float, float, float): {pt_file_name: pt_file_path, ...}, ...}\n",
    "    \"\"\"\n",
    "    # 使用 glob 匹配所有符合条件的子文件夹\n",
    "    pattern = os.path.join(folder_path, \"*\")\n",
    "    matched_folders = glob.glob(pattern)\n",
    "\n",
    "    # 定义正则表达式提取三个浮点数\n",
    "    regex = re.compile(regax_str)\n",
    "\n",
    "    # 创建结果字典\n",
    "    result_dict = {}\n",
    "\n",
    "    # 遍历匹配的文件夹\n",
    "    for folder in matched_folders:\n",
    "        # 提取三个浮点数\n",
    "        match = regex.search(os.path.basename(folder))\n",
    "        if match:\n",
    "            floats = tuple(map(float, match.groups()))  # 转换为浮点数元组\n",
    "\n",
    "            # 构建 evaluations 文件夹路径\n",
    "            print(os.path.basename(folder))\n",
    "            evaluations_path = os.path.join(folder, \"evaluations\")\n",
    "\n",
    "            # 检查 evaluations 文件夹是否存在\n",
    "            if os.path.exists(evaluations_path) and os.path.isdir(evaluations_path):\n",
    "                # 获取 evaluations 文件夹下的所有 .pt 文件\n",
    "                pt_files = glob.glob(os.path.join(evaluations_path, \"*.pt\"))\n",
    "\n",
    "                # 将 .pt 文件以文件名（不含扩展名）为键，存储到字典中\n",
    "                pt_dict = {}\n",
    "                for pt_file in pt_files:\n",
    "                    file_name = os.path.splitext(os.path.basename(pt_file))[\n",
    "                        0\n",
    "                    ]  # 去掉扩展名\n",
    "                    pt_dict[file_name] = pt_file  # 存储文件路径\n",
    "\n",
    "                # 将结果存储到主字典中\n",
    "                result_dict[floats] = pt_dict\n",
    "\n",
    "    sorted_result_dict = dict(sorted(result_dict.items()))\n",
    "\n",
    "    return sorted_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\t1e4\t0.0\t0.0\t\t21.72\t\t30.85\t\t33.86\t\n",
      "1.0\t1e4\t0.1\t0.0\t\t22.66\t\t31.29\t\t37.04\t\n",
      "1.0\t1e4\t0.5\t0.0\t\t15.73\t\t23.51\t\t31.79\t\n",
      "1.0\t1e4\t1.0\t0.0\t\t15.58\t\t24.01\t\t40.72\t\n",
      "1.0\t1e4\t1.5\t0.0\t\t14.71\t\t22.94\t\t41.73\t\n",
      "1.0\t1e4\t2.0\t0.0\t\t13.93\t\t21.94\t\t42.16\t\n",
      "1.0\t1e4\t5.0\t0.0\t\t16.60\t\t24.87\t\t44.45\t\n",
      "1.0\t1e4\t10.0\t0.0\t\t12.61\t\t19.64\t\t43.26\t\n",
      "1.0\t1e4\t0.0\t1.0\t\t22.81\t\t32.08\t\t36.79\t\n",
      "1.0\t1e4\t0.1\t1.0\t\t20.41\t\t28.51\t\t35.78\t\n",
      "1.0\t1e4\t0.5\t1.0\t\t17.67\t\t26.09\t\t35.07\t\n",
      "1.0\t1e4\t1.0\t1.0\t\t17.15\t\t25.77\t\t40.53\t\n",
      "1.0\t1e4\t1.5\t1.0\t\t14.97\t\t23.30\t\t42.16\t\n",
      "1.0\t1e4\t2.0\t1.0\t\t23.05\t\t31.77\t\t41.90\t\n",
      "1.0\t1e4\t5.0\t1.0\t\t15.74\t\t23.89\t\t45.02\t\n",
      "1.0\t1e4\t10.0\t1.0\t\t11.62\t\t18.42\t\t44.13   \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "original_txt = \"\"\"1.0\t1e4\t0.0\t0.0\t\t21.72\t\t30.85\t\t33.86\t\n",
    "1.0\t1e4\t0.1\t0.0\t\t22.66\t\t31.29\t\t37.04\t\n",
    "1.0\t1e4\t0.5\t0.0\t\t15.73\t\t23.51\t\t31.79\t\n",
    "1.0\t1e4\t1.0\t0.0\t\t15.58\t\t24.01\t\t40.72\t\n",
    "1.0\t1e4\t1.5\t0.0\t\t14.71\t\t22.94\t\t41.73\t\n",
    "1.0\t1e4\t2.0\t0.0\t\t13.93\t\t21.94\t\t42.16\t\n",
    "1.0\t1e4\t5.0\t0.0\t\t16.60\t\t24.87\t\t44.45\t\n",
    "1.0\t1e4\t10.0\t0.0\t\t12.61\t\t19.64\t\t43.26\t\n",
    "1.0\t1e4\t0.0\t1.0\t\t22.81\t\t32.08\t\t36.79\t\n",
    "1.0\t1e4\t0.1\t1.0\t\t20.41\t\t28.51\t\t35.78\t\n",
    "1.0\t1e4\t0.5\t1.0\t\t17.67\t\t26.09\t\t35.07\t\n",
    "1.0\t1e4\t1.0\t1.0\t\t17.15\t\t25.77\t\t40.53\t\n",
    "1.0\t1e4\t1.5\t1.0\t\t14.97\t\t23.30\t\t42.16\t\n",
    "1.0\t1e4\t2.0\t1.0\t\t23.05\t\t31.77\t\t41.90\t\n",
    "1.0\t1e4\t5.0\t1.0\t\t15.74\t\t23.89\t\t45.02\t\n",
    "1.0\t1e4\t10.0\t1.0\t\t11.62\t\t18.42\t\t44.13   \"\"\".split(\"\\n\")\n",
    "\n",
    "folder_path = \"../outputs/lambda_ablation\"\n",
    "result = collect_pt_files(folder_path, spss)\n",
    "\n",
    "for idx1, (floats, acc_path) in enumerate(result.items()):\n",
    "    # print(f\"Floats: {floats}\")\n",
    "    # print(f\"path: {acc_path}\")\n",
    "    adi_pack = torch.load(acc_path[\"adi_pack\"])\n",
    "    # print(adi_pack)\n",
    "    original_txt[idx1] += f\"\\t{adi_pack[\"avg_drop\"].item() * 100:.2f}\\t{adi_pack[\"avg_inc\"].item() * 100:.2f}\\t{adi_pack[\"avg_gain\"].item() * 100:.2f}\"\n",
    "    \n",
    "result = collect_pt_files(folder_path, cspss)\n",
    "\n",
    "for idx2, (floats, acc_path) in enumerate(result.items()):\n",
    "    # print(f\"Floats: {floats}\")\n",
    "    # print(f\"path: {acc_path}\")\n",
    "    adi_pack = torch.load(acc_path[\"adi_pack\"])\n",
    "    # print(adi_pack)\n",
    "    original_txt[idx1 + 1 + idx2] += f\"\\t{adi_pack[\"avg_drop\"].item() * 100:.2f}\\t{adi_pack[\"avg_inc\"].item() * 100:.2f}\\t{adi_pack[\"avg_gain\"].item() * 100:.2f}\"\n",
    "    \n",
    "print(\"\\n\".join(original_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "css_vl_cbm_train_simple_concepts\n",
      "PCBM-ViT-B-16-laion\n",
      "spss_vl_cbm_train_celebA_concepts_1.0_1.0_5.0\n",
      "PCBM-RN50\n",
      "dict_keys(['classes_segmentation_metric', 'concepts_segmentation_metric', 'weighted_cocnepts_metric', 'nec', 'accuracy', 'adi_pack'])\n",
      "{'nec5': 72.46000000000001, 'anec': 72.46000000000001}\n",
      "dict_keys(['concepts_avg_drop', 'concepts_avg_inc', 'concepts_avg_gain', 'classes_avg_drop', 'classes_avg_inc', 'classes_avg_gain'])\n",
      "dict_keys(['classes_segmentation_metric', 'concepts_segmentation_metric', 'weighted_cocnepts_metric', 'nec', 'accuracy', 'adi_pack'])\n",
      "{'nec5': 67.53, 'anec': 67.53}\n",
      "dict_keys(['concepts_avg_drop', 'concepts_avg_inc', 'concepts_avg_gain', 'classes_avg_drop', 'classes_avg_inc', 'classes_avg_gain'])\n",
      "dict_keys(['classes_segmentation_metric', 'concepts_segmentation_metric', 'weighted_cocnepts_metric', 'nec', 'accuracy', 'adi_pack'])\n",
      "{'nec5': 91.27, 'anec': 91.27}\n",
      "dict_keys(['concepts_avg_drop', 'concepts_avg_inc', 'concepts_avg_gain', 'classes_avg_drop', 'classes_avg_inc', 'classes_avg_gain'])\n",
      "dict_keys(['nec', 'accuracy', 'adi_pack'])\n",
      "{'nec5': 87.96000000000001, 'anec': 87.95}\n",
      "dict_keys(['concepts_avg_drop', 'concepts_avg_inc', 'concepts_avg_gain', 'classes_avg_drop', 'classes_avg_inc', 'classes_avg_gain'])\n",
      "PCBM-RN50\t72.46& 47.52& 72.46& 72.46& & 1.06& 1.15& 29.86& 25.54& 0.36& 0.28\n",
      "PCBM-ViT-B-16-laion\t67.53& 42.36& 67.53& 67.53& & 0.37& 0.37& 83.34& 82.68& 25.63& 22.96\n",
      "css_vl_cbm_train_simple_concepts\t91.27& 71.54& 91.27& 91.27& & 2.28& 3.10& 38.34& 28.31& 2.62& 1.99\n",
      "spss_vl_cbm_train_celebA_concepts_1.0_1.0_5.0\t87.94& 77.31& 87.96& 87.95& & 20.13& 17.05& 47.23& 47.36& 22.10& 18.64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import re\n",
    "import glob\n",
    "from utils.eval_utils import class_attribution_metric, attribution_metric\n",
    "from utils import RIVAL10_constants\n",
    "\n",
    "def collect_pt_files(folder_path):\n",
    "    \"\"\"\n",
    "    收集指定文件夹下所有符合条件的子文件夹中的 .pt 文件，并按三个浮点数组织成字典。\n",
    "\n",
    "    参数:\n",
    "        folder_path (str): 根文件夹路径。\n",
    "\n",
    "    返回:\n",
    "        dict: 格式为 {str: {pt_file_name: pt_file_path, ...}, ...}\n",
    "    \"\"\"\n",
    "    # 使用 glob 匹配所有符合条件的子文件夹\n",
    "    pattern = os.path.join(folder_path, \"*\")\n",
    "    matched_folders = glob.glob(pattern)\n",
    "\n",
    "    # 创建结果字典\n",
    "    result_dict = {}\n",
    "\n",
    "    # 遍历匹配的文件夹\n",
    "    for folder in matched_folders:\n",
    "        # 提取三个浮点数\n",
    "\n",
    "        # 构建 evaluations 文件夹路径\n",
    "        print(os.path.basename(folder))\n",
    "        evaluations_path = os.path.join(folder, \"evaluations\")\n",
    "\n",
    "        # 检查 evaluations 文件夹是否存在\n",
    "        if os.path.exists(evaluations_path) and os.path.isdir(evaluations_path):\n",
    "            # 获取 evaluations 文件夹下的所有 .pt 文件\n",
    "            pt_files = glob.glob(os.path.join(evaluations_path, \"*.pt\"))\n",
    "\n",
    "            # 将 .pt 文件以文件名（不含扩展名）为键，存储到字典中\n",
    "            pt_dict = {}\n",
    "            for pt_file in pt_files:\n",
    "                file_name = os.path.splitext(os.path.basename(pt_file))[\n",
    "                    0\n",
    "                ]  # 去掉扩展名\n",
    "                pt_dict[file_name] = pt_file  # 存储文件路径\n",
    "\n",
    "            # 将结果存储到主字典中\n",
    "            result_dict[os.path.basename(folder)] = pt_dict\n",
    "\n",
    "    sorted_result_dict = dict(sorted(result_dict.items()))\n",
    "\n",
    "    return sorted_result_dict\n",
    "\n",
    "folder_path = \"../outputs/celebA\"\n",
    "result = collect_pt_files(folder_path)\n",
    "\n",
    "adi_txt = []\n",
    "for file_name, pt_dict in result.items():\n",
    "    # print(f\"Floats: {floats}\")\n",
    "    print(pt_dict.keys())\n",
    "    adi_txt.append(f\"{file_name}\\t\")\n",
    "    for file_name in [\"accuracy\", \"nec\", \"adi_pack\"]:\n",
    "        if file_name == \"nec\":\n",
    "            metric = torch.load(pt_dict[file_name])\n",
    "            print(metric)\n",
    "            adi_txt[-1] += (f\"{metric[\"nec5\"]:.2f}& {metric[\"anec\"]:.2f}& \"\n",
    "            )\n",
    "        elif file_name == \"accuracy\":\n",
    "            metric = torch.load(pt_dict[file_name])\n",
    "            adi_txt[-1] += (f\"{metric[\"test_acc\"]:.2f}& {metric[\"test_concept_acc\"]:.2f}& \"\n",
    "            )\n",
    "        elif file_name == \"adi_pack\":\n",
    "            metric = torch.load(pt_dict[file_name])\n",
    "            print(metric.keys())\n",
    "            adi_txt[-1] += (f\"{metric[\"concepts_avg_drop\"].item() * 100:.2f}& {metric[\"classes_avg_drop\"].item() * 100:.2f}& {metric[\"concepts_avg_inc\"].item() * 100:.2f}& {metric[\"classes_avg_inc\"].item() * 100:.2f}& {metric[\"concepts_avg_gain\"].item() * 100:.2f}& {metric[\"classes_avg_gain\"].item() * 100:.2f}\")\n",
    "            \n",
    "print(\"\\n\".join(adi_txt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pcbm_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
