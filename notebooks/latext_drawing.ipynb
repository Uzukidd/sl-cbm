{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing adi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import re\n",
    "import glob\n",
    "from utils.eval_utils import class_attribution_metric, attribution_metric\n",
    "from utils import RIVAL10_constants\n",
    "\n",
    "spss = r\"^eval_spss_vl_cbm_train_simple_concepts_([\\d.]+)_([\\d.]+)_([\\d.]+)$\"\n",
    "cspss = r\"^eval_spss_vl_cbm_train_simple_concepts_([\\d.]+)_([\\d.]+)_([\\d.]+)_([\\d.]+)$\"\n",
    "# spss_softmax = r\"^spss_vl_cbm_train_simple_concepts_([\\d.]+)_([\\d.]+)_([\\d.]+)\"\n",
    "# cspss_softmax = (\n",
    "#     r\"^cspss_vl_cbm_train_simple_concepts_([\\d.]+)_([\\d.]+)_([\\d.]+)_([\\d.]+)\"\n",
    "# )\n",
    "\n",
    "\n",
    "def collect_pt_files(folder_path, regax_str: str):\n",
    "    \"\"\"\n",
    "    收集指定文件夹下所有符合条件的子文件夹中的 .pt 文件，并按三个浮点数组织成字典。\n",
    "\n",
    "    参数:\n",
    "        folder_path (str): 根文件夹路径。\n",
    "\n",
    "    返回:\n",
    "        dict: 格式为 {(float, float, float): {pt_file_name: pt_file_path, ...}, ...}\n",
    "    \"\"\"\n",
    "    # 使用 glob 匹配所有符合条件的子文件夹\n",
    "    pattern = os.path.join(folder_path, \"*\")\n",
    "    matched_folders = glob.glob(pattern)\n",
    "\n",
    "    # 定义正则表达式提取三个浮点数\n",
    "    regex = re.compile(regax_str)\n",
    "\n",
    "    # 创建结果字典\n",
    "    result_dict = {}\n",
    "\n",
    "    # 遍历匹配的文件夹\n",
    "    for folder in matched_folders:\n",
    "        # 提取三个浮点数\n",
    "        match = regex.search(os.path.basename(folder))\n",
    "        if match:\n",
    "            floats = tuple(map(float, match.groups()))  # 转换为浮点数元组\n",
    "\n",
    "            # 构建 evaluations 文件夹路径\n",
    "            print(os.path.basename(folder))\n",
    "            evaluations_path = os.path.join(folder, \"evaluations\")\n",
    "\n",
    "            # 检查 evaluations 文件夹是否存在\n",
    "            if os.path.exists(evaluations_path) and os.path.isdir(evaluations_path):\n",
    "                # 获取 evaluations 文件夹下的所有 .pt 文件\n",
    "                pt_files = glob.glob(os.path.join(evaluations_path, \"*.pt\"))\n",
    "\n",
    "                # 将 .pt 文件以文件名（不含扩展名）为键，存储到字典中\n",
    "                pt_dict = {}\n",
    "                for pt_file in pt_files:\n",
    "                    file_name = os.path.splitext(os.path.basename(pt_file))[\n",
    "                        0\n",
    "                    ]  # 去掉扩展名\n",
    "                    pt_dict[file_name] = pt_file  # 存储文件路径\n",
    "\n",
    "                # 将结果存储到主字典中\n",
    "                result_dict[floats] = pt_dict\n",
    "\n",
    "    sorted_result_dict = dict(sorted(result_dict.items()))\n",
    "\n",
    "    return sorted_result_dict\n",
    "\n",
    "\n",
    "def collect_acc_files(folder_path, regax_str: str):\n",
    "    \"\"\"\n",
    "    收集指定文件夹下所有符合条件的子文件夹中的 .pt 文件，并按三个浮点数组织成字典。\n",
    "\n",
    "    参数:\n",
    "        folder_path (str): 根文件夹路径。\n",
    "\n",
    "    返回:\n",
    "        dict: 格式为 {(float, float, float): exp_log.log\n",
    "    \"\"\"\n",
    "    # 使用 glob 匹配所有符合条件的子文件夹\n",
    "    pattern = os.path.join(folder_path, \"*\")\n",
    "    matched_folders = glob.glob(pattern)\n",
    "\n",
    "    # 定义正则表达式提取三个浮点数\n",
    "    regex = re.compile(regax_str)\n",
    "\n",
    "    # 创建结果字典\n",
    "    result_dict = {}\n",
    "\n",
    "    # 遍历匹配的文件夹\n",
    "    for folder in matched_folders:\n",
    "        # 提取三个浮点数\n",
    "        match = regex.search(os.path.basename(folder))\n",
    "        if match:\n",
    "            floats = tuple(map(float, match.groups()))  # 转换为浮点数元组\n",
    "\n",
    "            # 构建 evaluations 文件夹路径\n",
    "            print(os.path.basename(folder))\n",
    "            log_path = os.path.join(folder, \"exp_log.log\")\n",
    "\n",
    "            # 检查 evaluations 文件夹是否存在\n",
    "            if os.path.exists(log_path) and not os.path.isdir(log_path):\n",
    "                result_dict[floats] = log_path\n",
    "\n",
    "    sorted_result_dict = dict(sorted(result_dict.items()))\n",
    "\n",
    "    return sorted_result_dict\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_last_accuracies(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # 正则表达式匹配 \"Val Class Accuracy = xxx.xx\" 和 \"Val Concept Accuracy = xxx.xx\"\n",
    "    pattern = r\"Val Class Accuracy = (\\d+\\.\\d+).*?Val Concept Accuracy = (\\d+\\.\\d+)\"\n",
    "\n",
    "    matches = re.findall(pattern, text)\n",
    "\n",
    "    if matches:\n",
    "        # 取最后一个匹配的结果\n",
    "        last_class_acc, last_concept_acc = matches[-1]\n",
    "        return float(last_class_acc), float(last_concept_acc)\n",
    "    else:\n",
    "        return None  # 没有找到匹配项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_0.1\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_5.0\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_0.5\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_10.0\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_1.0\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_0.0\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_1.5\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_2.0\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_10.0_1.0\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_1.0_1.0\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_2.0_1.0\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_0.5_1.0\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_0.1_1.0\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_1.5_1.0\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_5.0_1.0\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_0.0_1.0\n",
      "1.0\t1e4\t0.0\t0.0\t\t21.72\t\t30.85\t\t33.86\t\n",
      "1.0\t1e4\t0.1\t0.0\t\t22.66\t\t31.29\t\t37.04\t\n",
      "1.0\t1e4\t0.5\t0.0\t\t15.73\t\t23.51\t\t31.79\t\n",
      "1.0\t1e4\t1.0\t0.0\t\t15.58\t\t24.01\t\t40.72\t\n",
      "1.0\t1e4\t1.5\t0.0\t\t14.71\t\t22.94\t\t41.73\t\n",
      "1.0\t1e4\t2.0\t0.0\t\t13.93\t\t21.94\t\t42.16\t\n",
      "1.0\t1e4\t5.0\t0.0\t\t16.60\t\t24.87\t\t44.45\t\n",
      "1.0\t1e4\t10.0\t0.0\t\t12.61\t\t19.64\t\t43.26\t\n",
      "1.0\t1e4\t0.0\t1.0\t\t22.81\t\t32.08\t\t36.79\t\n",
      "1.0\t1e4\t0.1\t1.0\t\t20.41\t\t28.51\t\t35.78\t\n",
      "1.0\t1e4\t0.5\t1.0\t\t17.67\t\t26.09\t\t35.07\t\n",
      "1.0\t1e4\t1.0\t1.0\t\t17.15\t\t25.77\t\t40.53\t\n",
      "1.0\t1e4\t1.5\t1.0\t\t14.97\t\t23.30\t\t42.16\t\n",
      "1.0\t1e4\t2.0\t1.0\t\t23.05\t\t31.77\t\t41.90\t\n",
      "1.0\t1e4\t5.0\t1.0\t\t15.74\t\t23.89\t\t45.02\t\n",
      "1.0\t1e4\t10.0\t1.0\t\t11.62\t\t18.42\t\t44.13\t\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"../outputs/lambda_ablation\"\n",
    "result = collect_pt_files(folder_path, spss)\n",
    "\n",
    "adi_txt = []\n",
    "\n",
    "for floats, pt_dict in result.items():\n",
    "    # print(f\"Floats: {floats}\")\n",
    "    for file_name, file_path in pt_dict.items():\n",
    "        if file_name.startswith(\"classes\"):\n",
    "            metric = class_attribution_metric(\"\", 0)\n",
    "            metric.load_from_path(file_path)\n",
    "            # print(metric.format_output(RIVAL10_constants._ALL_CLASSNAMES, latex=True))\n",
    "        elif file_name == \"concepts_segmentation_metric\":\n",
    "            metric = attribution_metric(\"\", 0, 0)\n",
    "            metric.load_from_path(file_path)\n",
    "            # print(f\"1.0\\t1e4\\t{floats[2]}\\t0.0\\t{adi_pack[\"avg_drop\"].item():.2f}\\t{adi_pack[\"avg_gain\"]:.2f}\\t{adi_pack[\"avg_inc\"]:.2f}\")\n",
    "            adi_txt.append(f\"1.0\\t1e4\\t{floats[2]}\\t0.0\\t\" +\n",
    "                metric.format_output(\n",
    "                    RIVAL10_constants._ALL_CLASSNAMES,\n",
    "                    RIVAL10_constants._ALL_ATTRS,\n",
    "                    latex=True,\n",
    "                    sep=\"\\t\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "folder_path = \"../outputs/lambda_ablation\"\n",
    "result = collect_pt_files(folder_path, cspss)\n",
    "\n",
    "for floats, pt_dict in result.items():\n",
    "    # print(f\"Floats: {floats}\")\n",
    "    for file_name, file_path in pt_dict.items():\n",
    "        if file_name.startswith(\"classes\"):\n",
    "            metric = class_attribution_metric(\"\", 0)\n",
    "            metric.load_from_path(file_path)\n",
    "            # print(metric.format_output(RIVAL10_constants._ALL_CLASSNAMES, latex=True))\n",
    "        elif file_name == \"concepts_segmentation_metric\":\n",
    "            metric = attribution_metric(\"\", 0, 0)\n",
    "            metric.load_from_path(file_path)\n",
    "            # print(f\"1.0\\t1e4\\t{floats[2]}\\t0.0\\t{adi_pack[\"avg_drop\"].item():.2f}\\t{adi_pack[\"avg_gain\"]:.2f}\\t{adi_pack[\"avg_inc\"]:.2f}\")\n",
    "            adi_txt.append(f\"1.0\\t1e4\\t{floats[2]}\\t1.0\\t\" +\n",
    "                metric.format_output(\n",
    "                    RIVAL10_constants._ALL_CLASSNAMES,\n",
    "                    RIVAL10_constants._ALL_ATTRS,\n",
    "                    latex=True,\n",
    "                    sep=\"\\t\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "print(\"\\n\".join(adi_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "\n",
    "spss = r\"^eval_spss_vl_cbm_train_simple_concepts_([\\d.]+)_([\\d.]+)_([\\d.]+)$\"\n",
    "cspss = r\"^eval_spss_vl_cbm_train_simple_concepts_([\\d.]+)_([\\d.]+)_([\\d.]+)_([\\d.]+)$\"\n",
    "spss_softmax = r\"^eval_spss_vl_cbm_train_simple_concepts_([\\d.]+)_([\\d.]+)_([\\d.]+)$\"\n",
    "cspss_softmax = (\n",
    "    r\"^eval_cspss_vl_cbm_train_simple_concepts_([\\d.]+)_([\\d.]+)_([\\d.]+)_([\\d.]+)$\"\n",
    ")\n",
    "\n",
    "\n",
    "def collect_pt_files(folder_path, regax_str: str):\n",
    "    \"\"\"\n",
    "    收集指定文件夹下所有符合条件的子文件夹中的 .pt 文件，并按三个浮点数组织成字典。\n",
    "\n",
    "    参数:\n",
    "        folder_path (str): 根文件夹路径。\n",
    "\n",
    "    返回:\n",
    "        dict: 格式为 {(float, float, float): {pt_file_name: pt_file_path, ...}, ...}\n",
    "    \"\"\"\n",
    "    # 使用 glob 匹配所有符合条件的子文件夹\n",
    "    pattern = os.path.join(folder_path, \"*\")\n",
    "    matched_folders = glob.glob(pattern)\n",
    "\n",
    "    # 定义正则表达式提取三个浮点数\n",
    "    regex = re.compile(regax_str)\n",
    "\n",
    "    # 创建结果字典\n",
    "    result_dict = {}\n",
    "\n",
    "    # 遍历匹配的文件夹\n",
    "    for folder in matched_folders:\n",
    "        # 提取三个浮点数\n",
    "        match = regex.search(os.path.basename(folder))\n",
    "        if match:\n",
    "            floats = tuple(map(float, match.groups()))  # 转换为浮点数元组\n",
    "\n",
    "            # 构建 evaluations 文件夹路径\n",
    "            print(os.path.basename(folder))\n",
    "            evaluations_path = os.path.join(folder, \"evaluations\")\n",
    "\n",
    "            # 检查 evaluations 文件夹是否存在\n",
    "            if os.path.exists(evaluations_path) and os.path.isdir(evaluations_path):\n",
    "                # 获取 evaluations 文件夹下的所有 .pt 文件\n",
    "                pt_files = glob.glob(os.path.join(evaluations_path, \"*.pt\"))\n",
    "\n",
    "                # 将 .pt 文件以文件名（不含扩展名）为键，存储到字典中\n",
    "                pt_dict = {}\n",
    "                for pt_file in pt_files:\n",
    "                    file_name = os.path.splitext(os.path.basename(pt_file))[\n",
    "                        0\n",
    "                    ]  # 去掉扩展名\n",
    "                    pt_dict[file_name] = pt_file  # 存储文件路径\n",
    "\n",
    "                # 将结果存储到主字典中\n",
    "                result_dict[floats] = pt_dict\n",
    "\n",
    "    sorted_result_dict = dict(sorted(result_dict.items()))\n",
    "\n",
    "    return sorted_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_0.1\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_5.0\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_0.5\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_10.0\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_1.0\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_0.0\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_1.5\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_2.0\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_10.0_1.0\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_1.0_1.0\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_2.0_1.0\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_0.5_1.0\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_0.1_1.0\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_1.5_1.0\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_5.0_1.0\n",
      "eval_spss_vl_cbm_train_simple_concepts_1.0_1.0_0.0_1.0\n",
      "1.0\t1e4\t0.0\t0.0\t\t21.72\t\t30.85\t\t33.86\t\t3.67\t50.57\t13.57\n",
      "1.0\t1e4\t0.1\t0.0\t\t22.66\t\t31.29\t\t37.04\t\t3.47\t50.03\t14.16\n",
      "1.0\t1e4\t0.5\t0.0\t\t15.73\t\t23.51\t\t31.79\t\t3.68\t49.12\t15.20\n",
      "1.0\t1e4\t1.0\t0.0\t\t15.58\t\t24.01\t\t40.72\t\t3.96\t48.36\t15.90\n",
      "1.0\t1e4\t1.5\t0.0\t\t14.71\t\t22.94\t\t41.73\t\t3.93\t48.36\t15.44\n",
      "1.0\t1e4\t2.0\t0.0\t\t13.93\t\t21.94\t\t42.16\t\t3.91\t47.55\t15.25\n",
      "1.0\t1e4\t5.0\t0.0\t\t16.60\t\t24.87\t\t44.45\t\t3.61\t47.69\t13.15\n",
      "1.0\t1e4\t10.0\t0.0\t\t12.61\t\t19.64\t\t43.26\t\t4.54\t42.35\t10.26\n",
      "1.0\t1e4\t0.0\t1.0\t\t22.81\t\t32.08\t\t36.79\t\t2.66\t50.33\t9.26\n",
      "1.0\t1e4\t0.1\t1.0\t\t20.41\t\t28.51\t\t35.78\t\t2.86\t50.30\t10.20\n",
      "1.0\t1e4\t0.5\t1.0\t\t17.67\t\t26.09\t\t35.07\t\t2.90\t48.49\t10.88\n",
      "1.0\t1e4\t1.0\t1.0\t\t17.15\t\t25.77\t\t40.53\t\t3.12\t47.67\t10.97\n",
      "1.0\t1e4\t1.5\t1.0\t\t14.97\t\t23.30\t\t42.16\t\t3.15\t47.36\t10.96\n",
      "1.0\t1e4\t2.0\t1.0\t\t23.05\t\t31.77\t\t41.90\t\t2.75\t48.25\t10.57\n",
      "1.0\t1e4\t5.0\t1.0\t\t15.74\t\t23.89\t\t45.02\t\t2.94\t47.67\t9.81\n",
      "1.0\t1e4\t10.0\t1.0\t\t11.62\t\t18.42\t\t44.13   \t3.36\t42.87\t7.52\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "original_txt = \"\"\"1.0\t1e4\t0.0\t0.0\t\t21.72\t\t30.85\t\t33.86\t\n",
    "1.0\t1e4\t0.1\t0.0\t\t22.66\t\t31.29\t\t37.04\t\n",
    "1.0\t1e4\t0.5\t0.0\t\t15.73\t\t23.51\t\t31.79\t\n",
    "1.0\t1e4\t1.0\t0.0\t\t15.58\t\t24.01\t\t40.72\t\n",
    "1.0\t1e4\t1.5\t0.0\t\t14.71\t\t22.94\t\t41.73\t\n",
    "1.0\t1e4\t2.0\t0.0\t\t13.93\t\t21.94\t\t42.16\t\n",
    "1.0\t1e4\t5.0\t0.0\t\t16.60\t\t24.87\t\t44.45\t\n",
    "1.0\t1e4\t10.0\t0.0\t\t12.61\t\t19.64\t\t43.26\t\n",
    "1.0\t1e4\t0.0\t1.0\t\t22.81\t\t32.08\t\t36.79\t\n",
    "1.0\t1e4\t0.1\t1.0\t\t20.41\t\t28.51\t\t35.78\t\n",
    "1.0\t1e4\t0.5\t1.0\t\t17.67\t\t26.09\t\t35.07\t\n",
    "1.0\t1e4\t1.0\t1.0\t\t17.15\t\t25.77\t\t40.53\t\n",
    "1.0\t1e4\t1.5\t1.0\t\t14.97\t\t23.30\t\t42.16\t\n",
    "1.0\t1e4\t2.0\t1.0\t\t23.05\t\t31.77\t\t41.90\t\n",
    "1.0\t1e4\t5.0\t1.0\t\t15.74\t\t23.89\t\t45.02\t\n",
    "1.0\t1e4\t10.0\t1.0\t\t11.62\t\t18.42\t\t44.13   \"\"\".split(\"\\n\")\n",
    "\n",
    "folder_path = \"../outputs/lambda_ablation\"\n",
    "result = collect_pt_files(folder_path, spss)\n",
    "\n",
    "for idx1, (floats, acc_path) in enumerate(result.items()):\n",
    "    # print(f\"Floats: {floats}\")\n",
    "    # print(f\"path: {acc_path}\")\n",
    "    adi_pack = torch.load(acc_path[\"adi_pack\"])\n",
    "    # print(adi_pack)\n",
    "    original_txt[idx1] += f\"\\t{adi_pack[\"avg_drop\"].item() * 100:.2f}\\t{adi_pack[\"avg_inc\"].item() * 100:.2f}\\t{adi_pack[\"avg_gain\"].item() * 100:.2f}\"\n",
    "    \n",
    "result = collect_pt_files(folder_path, cspss)\n",
    "\n",
    "for idx2, (floats, acc_path) in enumerate(result.items()):\n",
    "    # print(f\"Floats: {floats}\")\n",
    "    # print(f\"path: {acc_path}\")\n",
    "    adi_pack = torch.load(acc_path[\"adi_pack\"])\n",
    "    # print(adi_pack)\n",
    "    original_txt[idx1 + 1 + idx2] += f\"\\t{adi_pack[\"avg_drop\"].item() * 100:.2f}\\t{adi_pack[\"avg_inc\"].item() * 100:.2f}\\t{adi_pack[\"avg_gain\"].item() * 100:.2f}\"\n",
    "    \n",
    "print(\"\\n\".join(original_txt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pcbm_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
