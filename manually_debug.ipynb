{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from pcbm.data import get_dataset\n",
    "from pcbm.concepts import ConceptBank\n",
    "from pcbm.models import PosthocLinearCBM, get_model\n",
    "from pcbm.training_tools import load_or_compute_projections\n",
    "\n",
    "UNIVERSAL_SEED = 2024\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 4\n",
    "CONCEPT_BANK_PATH = \"/home/ksas/Public/datasets/cifar10_concept_bank/multimodal_concept_clip:RN50_cifar10_recurse:1.pkl\"\n",
    "OUT_PUT_DIR_PATH = \"exps/test\"\n",
    "CKPT_PATH = \"data/ckpt/CIFAR_10/pcbm_cifar10__clip:RN50__multimodal_concept_clip:RN50_cifar10_recurse:1__lam:0.0002__alpha:0.99__seed:42.ckpt\"\n",
    "DATASET_PATH = \"/home/ksas/Public/datasets/cifar10_concept_bank\"\n",
    "BACKBONE_NAME = \"clip:ViT-B/32\"\n",
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "set_random_seed(UNIVERSAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_concepts = pkl.load(open(CONCEPT_BANK_PATH, 'rb'))\n",
    "all_concept_names = list(all_concepts.keys())\n",
    "print(f\"Bank path: {CONCEPT_BANK_PATH}. {len(all_concept_names)} concepts will be used.\")\n",
    "concept_bank = ConceptBank(all_concepts, DEVICE)\n",
    "\n",
    "import clip\n",
    "clip_backbone_name = BACKBONE_NAME.split(\":\")[1]\n",
    "backbone, preprocess = clip.load(clip_backbone_name, device=DEVICE, download_root=\"/home/ksas/Public/model_zoo/clip\")\n",
    "backbone = backbone.eval()\n",
    "backbone = backbone.float()\n",
    "model = None\n",
    "\n",
    "backbone = backbone.to(DEVICE)\n",
    "backbone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cifar10_concept.txt\", \"w+\") as input_stream:\n",
    "    for idx, concept_name in enumerate(concept_bank.concept_info.concept_names):\n",
    "        input_stream.write(f\"{idx}\\t-{concept_name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posthoc_layer:PosthocLinearCBM = torch.load(CKPT_PATH, map_location=DEVICE)\n",
    "print(posthoc_layer.analyze_classifier(k=5))\n",
    "print(posthoc_layer.names)\n",
    "print(posthoc_layer.names.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from pcbm.learn_concepts_multimodal import *\n",
    "trainset = datasets.CIFAR10(root=DATASET_PATH, train=True,\n",
    "                            download=True, transform=preprocess)\n",
    "testset = datasets.CIFAR10(root=DATASET_PATH, train=False,\n",
    "                            download=True, transform=preprocess)\n",
    "classes = trainset.classes\n",
    "class_to_idx = {c: i for (i,c) in enumerate(classes)}\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "                                    shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(images:torch.Tensor):\n",
    "    import torch\n",
    "    import torchvision\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # 使用 torchvision.utils.make_grid 将 64 张图片排列成 8x8 的网格\n",
    "    grid_img = torchvision.utils.make_grid(images, nrow=8, normalize=True)\n",
    "\n",
    "    # 转换为 NumPy 格式以便用 matplotlib 显示\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))  # 转换为 [H, W, C]\n",
    "    plt.axis('off')  # 隐藏坐标轴\n",
    "    plt.show()\n",
    "\n",
    "for idx, data in enumerate(train_loader):\n",
    "    print(data.__len__())\n",
    "    print(f\"x: {data[0].size()}\")\n",
    "    print(f\"y: {data[1].size()}\")\n",
    "    batch_X, batch_Y = data\n",
    "    batch_X = batch_X.to(DEVICE)\n",
    "    batch_Y = batch_Y.to(DEVICE)\n",
    "    \n",
    "    batch_X.requires_grad_(True)\n",
    "    embeddings = backbone.encode_image(batch_X)\n",
    "    projs = posthoc_layer.compute_dist(embeddings)\n",
    "    predicted_Y = posthoc_layer.forward_projs(projs)\n",
    "    accuracy = (predicted_Y.argmax(1) == batch_Y).float().mean().item()\n",
    "    \n",
    "    _, topk_indices = torch.topk(projs, 5, dim=1)\n",
    "    topk_concept = [[posthoc_layer.names[idx] for idx in row] for row in topk_indices]\n",
    "\n",
    "    \n",
    "    show_image(batch_X.detach().cpu())\n",
    "    print(f\"embeddings: {embeddings.size()}\")\n",
    "    print(f\"projections: {projs.size()}\")\n",
    "    print(f\"predicted_Y: {predicted_Y.size()}\")\n",
    "    print(f\"accuracy: {accuracy}\")\n",
    "    # accuracy_idx.append(accuracy)\n",
    "    import pdb; pdb.set_trace()\n",
    "    \n",
    "# print(accuracy_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import os\n",
    "from constants import dataset_cosntants\n",
    "from pcbm.data.cub import CUBConceptDataset, get_concept_dicts\n",
    "from pcbm.concepts import ConceptBank\n",
    "\n",
    "CUB_CONCEPT_BANK_PATH =  \"/home/ksas/Public/datasets/cub_concept_bank/cub_resnet18_cub_0.1_100.pkl\"\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "TRAIN_PKL = os.path.join(dataset_cosntants.CUB_PROCESSED_DIR, \"train.pkl\")\n",
    "metadata = pkl.load(open(TRAIN_PKL, \"rb\"))\n",
    "\n",
    "concept_info = get_concept_dicts(metadata=metadata)\n",
    "concept_info[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(concept_info.__len__())\n",
    "print(concept_info[0][0].__len__())\n",
    "print(concept_info[0][1].__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Layer Grad CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "from clip.model import CLIP, ModifiedResNet, VisionTransformer\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "BACKBONE_NAME = \"clip:ViT-B/32\"\n",
    "DEVICE = \"cuda\"\n",
    "clip_backbone_name = BACKBONE_NAME.split(\":\")[1]\n",
    "backbone, preprocess = clip.load(clip_backbone_name, device=DEVICE, download_root=\"/home/ksas/Public/model_zoo/clip\")\n",
    "backbone = backbone.eval()\n",
    "backbone = backbone.float()\n",
    "normalizer = transforms.Compose(preprocess.transforms[-1:])\n",
    "preprocess = transforms.Compose(preprocess.transforms[:-1])\n",
    "print(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.explain_utils import *\n",
    "from captum.attr import visualization, GradientAttribution, LayerAttribution\n",
    "\n",
    "image_attn_blocks = list(dict(backbone.visual.transformer.resblocks.named_children()).values())\n",
    "last_blocks = image_attn_blocks[-1].ln_1\n",
    "layer_grad_cam = layer_grad_cam_vit(backbone,\n",
    "                                last_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_utils import *\n",
    "\n",
    "image:torch.Tensor = preprocess(Image.open(\"data/images/cat_and_dog.jpg\")).unsqueeze(0).to(DEVICE)\n",
    "text = clip.tokenize([\"car\", \"a dog\", \"a cat\"]).to(DEVICE)\n",
    "print(backbone(normalizer(image), text))\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 1, additional_args = {\"text\": text})\n",
    "print(attributions)\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=False,\n",
    "        save_to=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image:torch.Tensor = preprocess(Image.open(\"data/images/multi_dog_and_cat.jpg\")).unsqueeze(0).to(DEVICE)\n",
    "text = clip.tokenize([\"cat and dog\", \"dog\", \"cat\", \"many dogs\", \"many cats\", \"eyees\"]).to(DEVICE)\n",
    "print(backbone(normalizer(image), text))\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 2, additional_args = {\"text\": text})\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=False,\n",
    "        save_to=None)\n",
    "\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 5, additional_args = {\"text\": text})\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=False,\n",
    "        save_to=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image:torch.Tensor = preprocess(Image.open(\"data/layer_grad_cam/propellers_images/946-original_image.jpg\")).unsqueeze(0).to(DEVICE)\n",
    "text = clip.tokenize([\"airplane\", \"propellers\", \"landing gear\"]).to(DEVICE)\n",
    "print(backbone(normalizer(image), text))\n",
    "\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 0, additional_args = {\"text\": text})\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=True,\n",
    "        save_to=None)\n",
    "\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 1, additional_args = {\"text\": text})\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=True,\n",
    "        save_to=None)\n",
    "\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 2, additional_args = {\"text\": text})\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=True,\n",
    "        save_to=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image:torch.Tensor = preprocess(Image.open(\"data/images/dog_cat.jpg\")).unsqueeze(0).to(DEVICE)\n",
    "text = clip.tokenize([\"car\", \"dog\", \"cat\"]).to(DEVICE)\n",
    "print(backbone(normalizer(image), text))\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 2, additional_args = {\"text\": text})\n",
    "print(attributions)\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=False,\n",
    "        save_to=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image:torch.Tensor = preprocess(Image.open(\"data/images/glasses.png\")).unsqueeze(0).to(DEVICE)\n",
    "text = clip.tokenize([\"man with eyeglasses\"]).to(DEVICE)\n",
    "print(backbone(normalizer(image), text))\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 0, additional_args = {\"text\": text})\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=True,\n",
    "        save_to=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image:torch.Tensor = preprocess(Image.open(\"data/images/airplane.jpg\")).unsqueeze(0).to(DEVICE)\n",
    "text = clip.tokenize([\"propellers\",\n",
    "                      \"heavier-than-air craft\",\n",
    "                      \"fuselage\",\n",
    "                      \"accelerator\",\n",
    "                      \"landing gear\"]).to(DEVICE)\n",
    "print(backbone(normalizer(image), text))\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 4, additional_args = {\"text\": text})\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=True,\n",
    "        save_to=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test OPEN_CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai', cache_dir=\"/home/ksas/Public/model_zoo/clip\")\n",
    "model.eval()  # model in train mode by default, impacts some models with BatchNorm or stochastic depth active\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "\n",
    "image = preprocess(Image.open(\"data/images/cat_and_dog.jpg\")).unsqueeze(0)\n",
    "text = tokenizer([\"a diagram\", \"a dog\", \"a cat\"])\n",
    "print(preprocess)\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Label probs:\", text_probs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from constants import dataset_constants\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "trainset = datasets.CIFAR10(root=dataset_constants.CIFAR10_DIR, train=True,\n",
    "                            download=False, transform=None)\n",
    "testset = datasets.CIFAR10(root=dataset_constants.CIFAR10_DIR, train=False,\n",
    "                            download=False, transform=None)\n",
    "classes = trainset.classes\n",
    "class_to_idx = {c: i for (i,c) in enumerate(classes)}\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(idx_to_class)\n",
    "plt.imshow(trainset[100][0])\n",
    "plt.axis('off')  # 隐藏坐标轴\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "CIFAR10_TRAINING_PATH = \"/home/ksas/Public/datasets/cifar10_concept_bank/training_imgs\"\n",
    "os.makedirs(CIFAR10_TRAINING_PATH, exist_ok=True)\n",
    "for idx, (image, cls) in tqdm(enumerate(trainset), total=trainset.__len__()):\n",
    "    image.save(os.path.join(CIFAR10_TRAINING_PATH, f\"{idx:05d}_{idx_to_class[cls]}.png\"), format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR10_TESTING_PATH = \"/home/ksas/Public/datasets/cifar10_concept_bank/testing_imgs\"\n",
    "os.makedirs(CIFAR10_TESTING_PATH, exist_ok=True)\n",
    "for idx, (image, cls) in tqdm(enumerate(testset), total=testset.__len__()):\n",
    "    image.save(os.path.join(CIFAR10_TESTING_PATH, f\"{idx:05d}_{idx_to_class[cls]}.png\"), format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 读取 CSV 文件\n",
    "df = pd.read_csv('/home/ksas/Public/datasets/cifar10_concept_bank/training_imgs.csv')\n",
    "\n",
    "# 打印前几行，验证读取是否成功\n",
    "print(df.head())\n",
    "print(df[\"filepath\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP full-shot evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_clip import get_input_dtype, get_tokenizer, build_zero_shot_classifier, \\\n",
    "    IMAGENET_CLASSNAMES, OPENAI_IMAGENET_TEMPLATES\n",
    "from open_clip_train.precision import get_autocast\n",
    "\n",
    "CIFAR10_CLASSNAME = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    pred = output.topk(max(topk), 1, True, True)[1].t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    return [float(correct[:k].reshape(-1).float().sum(0, keepdim=True).cpu().numpy()) for k in topk]\n",
    "\n",
    "\n",
    "def run(model, classifier, dataloader):\n",
    "    device = torch.device(\"cuda\")\n",
    "    autocast = get_autocast(\"amp\", device_type=device.type)\n",
    "    input_dtype = get_input_dtype(\"amp\")\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        top1, top5, n = 0., 0., 0.\n",
    "        for images, target in tqdm(dataloader, unit_scale=64):\n",
    "            images = images.to(device=device, dtype=input_dtype)\n",
    "            target = target.to(device)\n",
    "\n",
    "            with autocast():\n",
    "                # predict\n",
    "                output = model(image=images)\n",
    "                image_features = output['image_features'] if isinstance(output, dict) else output[0]\n",
    "                logits = 100. * image_features @ classifier\n",
    "\n",
    "            # measure accuracy\n",
    "            acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n",
    "            top1 += acc1\n",
    "            top5 += acc5\n",
    "            n += images.size(0)\n",
    "\n",
    "    top1 = (top1 / n)\n",
    "    top5 = (top5 / n)\n",
    "    return top1, top5\n",
    " \n",
    "def zero_shot_eval(model, dataloader, tokenizer):\n",
    "    print('Starting zero-shot imagenet.')\n",
    "    print('Building zero-shot classifier')\n",
    "    device = torch.device(\"cuda\")\n",
    "    autocast = get_autocast(\"amp\", device_type=device.type)\n",
    "    with autocast():\n",
    "        classifier = build_zero_shot_classifier(\n",
    "            model,\n",
    "            tokenizer=tokenizer,\n",
    "            classnames=CIFAR10_CLASSNAME,\n",
    "            templates= (\n",
    "                lambda c: f'a picture of a {c}.',\n",
    "            ),\n",
    "            num_classes_per_batch=10,\n",
    "            device=device,\n",
    "            use_tqdm=True,\n",
    "        )\n",
    "\n",
    "    print('Using classifier')\n",
    "    results = {}\n",
    "    top1, top5 = run(model, classifier, dataloader)\n",
    "    print(f\"top1: {top1}, top5: {top5}\")\n",
    "    print('Finished zero-shot imagenet.')\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "MODEL_NAME = 'ViT-B-32'\n",
    "ORIGINAL_CKPT_PATH = \"/home/ksas/Public/model_zoo/clip/ViT-B-32.pt\"\n",
    "REFINED_CKPT_PATH = \"/home/ksas/uzuki_space/open_clip/src/logs/2024_10_24-10_48_09-model_ViT-B-32-lr_1e-05-b_128-j_3-p_amp/checkpoints/epoch_2.pt\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(MODEL_NAME, pretrained=ORIGINAL_CKPT_PATH, cache_dir=\"/home/ksas/Public/model_zoo/clip\")\n",
    "model.cuda().eval()\n",
    "tokenizer = open_clip.get_tokenizer(MODEL_NAME)\n",
    "\n",
    "from common_utils import *\n",
    "\n",
    "trainset, testset, class_to_idx, idx_to_class, train_loader, test_loader = load_dataset(dataset_configure(\n",
    "            dataset = 'cifar10',\n",
    "            batch_size = 64,\n",
    "            num_workers = 4,\n",
    "        ), preprocess)\n",
    "\n",
    "zero_shot_eval(model, train_loader, tokenizer)\n",
    "zero_shot_eval(model, test_loader, tokenizer)\n",
    "\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(MODEL_NAME, pretrained=REFINED_CKPT_PATH, cache_dir=\"/home/ksas/Public/model_zoo/clip\")\n",
    "model.cuda().eval()\n",
    "\n",
    "zero_shot_eval(model, train_loader, tokenizer)\n",
    "zero_shot_eval(model, test_loader, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn import net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize CUB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "cub_mean_pxs = np.array([0.5, 0.5, 0.5])\n",
    "cub_std_pxs = np.array([2., 2., 2.])\n",
    "preprocess = transforms.Compose([\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(cub_mean_pxs, cub_std_pxs)\n",
    "        ])\n",
    "\n",
    "concept_bank = load_concept_bank(concept_bank_configure(\n",
    "            concept_bank = \"/home/ksas/Public/datasets/cub_concept_bank/cub_resnet18_cub_0.1_100.pkl\",\n",
    "            device = torch.device(\"cuda\")\n",
    "        ))\n",
    "backbone, preprocess = load_backbone(backbone_configure(\n",
    "    backbone_name=\"resnet18_cub\",\n",
    "    backbone_ckpt = \"/home/ksas/Public/model_zoo/resnet_cub\",\n",
    "    device = torch.device(\"cuda\")\n",
    "), full_load=True)\n",
    "normalizer = transforms.Compose(preprocess.transforms[-1:])\n",
    "preprocess = transforms.Compose(preprocess.transforms[:-1])\n",
    "\n",
    "posthoc_layer = load_pcbm(pcbm_configure(\n",
    "        pcbm_ckpt = \"data/ckpt/CUB/pcbm_cub__resnet18_cub__cub_resnet18_cub_0__lam:0.0002__alpha:0.99__seed:42.ckpt\",\n",
    "        device = torch.device(\"cuda\")\n",
    "))\n",
    "\n",
    "trainset, testset, class_to_idx, idx_to_class, train_loader, test_loader= load_dataset(dataset_configure(\n",
    "        dataset = \"cub\",\n",
    "        batch_size = 8,\n",
    "        num_workers = 4,\n",
    "), preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils.visual_utils import show_image\n",
    "\n",
    "totall_accuracy = []\n",
    "for idx, data in enumerate(tqdm(train_loader)):\n",
    "    batch_X, batch_Y = data\n",
    "    # show_image(batch_X)\n",
    "    batch_X:torch.Tensor = batch_X.to(torch.device(\"cuda\"))\n",
    "    batch_Y:torch.Tensor = batch_Y.to(torch.device(\"cuda\"))\n",
    "    totall_accuracy.append((backbone(batch_X).argmax(1) == batch_Y).float().mean().item())\n",
    "    # concept = posthoc_layer.compute_dist(backbone.encode_image(batch_X))\n",
    "    \n",
    "    # import pdb; pdb.set_trace()\n",
    "    \n",
    "print(np.array(totall_accuracy).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize CUB concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from pcbm.data.constants import CUB_PROCESSED_DIR\n",
    "from pcbm.data.cub import CUBConceptDataset, get_concept_dicts\n",
    "\n",
    "TRAIN_PKL = os.path.join(CUB_PROCESSED_DIR, \"train.pkl\")\n",
    "metadata = pickle.load(open(TRAIN_PKL, \"rb\"))\n",
    "\n",
    "concept_info = get_concept_dicts(metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metadata[0].keys())\n",
    "print(metadata[0][\"attribute_certainty\"])\n",
    "\n",
    "img_1 = metadata[800]\n",
    "img_2 = None\n",
    "print(img_1[\"attribute_label\"].__len__())\n",
    "for img in metadata[1:]:\n",
    "    if img_1[\"class_label\"] == img[\"class_label\"]:\n",
    "        print(img_1[\"attribute_label\"] == img[\"attribute_label\"])\n",
    "        print(img_1[\"attribute_certainty\"] == img[\"attribute_certainty\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pcbm_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
