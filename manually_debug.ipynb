{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from pcbm.data import get_dataset\n",
    "from pcbm.concepts import ConceptBank\n",
    "from pcbm.models import PosthocLinearCBM, get_model\n",
    "from pcbm.training_tools import load_or_compute_projections\n",
    "\n",
    "UNIVERSAL_SEED = 2024\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 4\n",
    "CONCEPT_BANK_PATH = \"/home/ksas/Public/datasets/cifar10_concept_bank/multimodal_concept_clip:RN50_cifar10_recurse:1.pkl\"\n",
    "OUT_PUT_DIR_PATH = \"exps/test\"\n",
    "CKPT_PATH = \"data/ckpt/CIFAR_10/pcbm_cifar10__clip:RN50__multimodal_concept_clip:RN50_cifar10_recurse:1__lam:0.0002__alpha:0.99__seed:42.ckpt\"\n",
    "DATASET_PATH = \"/home/ksas/Public/datasets/cifar10_concept_bank\"\n",
    "BACKBONE_NAME = \"clip:ViT-B/32\"\n",
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "set_random_seed(UNIVERSAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_concepts = pkl.load(open(CONCEPT_BANK_PATH, 'rb'))\n",
    "all_concept_names = list(all_concepts.keys())\n",
    "print(f\"Bank path: {CONCEPT_BANK_PATH}. {len(all_concept_names)} concepts will be used.\")\n",
    "concept_bank = ConceptBank(all_concepts, DEVICE)\n",
    "\n",
    "import clip\n",
    "clip_backbone_name = BACKBONE_NAME.split(\":\")[1]\n",
    "backbone, preprocess = clip.load(clip_backbone_name, device=DEVICE, download_root=\"/home/ksas/Public/model_zoo/clip\")\n",
    "backbone = backbone.eval()\n",
    "backbone = backbone.float()\n",
    "model = None\n",
    "\n",
    "backbone = backbone.to(DEVICE)\n",
    "backbone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cifar10_concept.txt\", \"w+\") as input_stream:\n",
    "    for idx, concept_name in enumerate(concept_bank.concept_info.concept_names):\n",
    "        input_stream.write(f\"{idx}\\t-{concept_name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posthoc_layer:PosthocLinearCBM = torch.load(CKPT_PATH, map_location=DEVICE)\n",
    "print(posthoc_layer.analyze_classifier(k=5))\n",
    "print(posthoc_layer.names)\n",
    "print(posthoc_layer.names.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from pcbm.learn_concepts_multimodal import *\n",
    "trainset = datasets.CIFAR10(root=DATASET_PATH, train=True,\n",
    "                            download=True, transform=preprocess)\n",
    "testset = datasets.CIFAR10(root=DATASET_PATH, train=False,\n",
    "                            download=True, transform=preprocess)\n",
    "classes = trainset.classes\n",
    "class_to_idx = {c: i for (i,c) in enumerate(classes)}\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "                                    shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(images:torch.Tensor):\n",
    "    import torch\n",
    "    import torchvision\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # 使用 torchvision.utils.make_grid 将 64 张图片排列成 8x8 的网格\n",
    "    grid_img = torchvision.utils.make_grid(images, nrow=8, normalize=True)\n",
    "\n",
    "    # 转换为 NumPy 格式以便用 matplotlib 显示\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))  # 转换为 [H, W, C]\n",
    "    plt.axis('off')  # 隐藏坐标轴\n",
    "    plt.show()\n",
    "\n",
    "for idx, data in enumerate(train_loader):\n",
    "    print(data.__len__())\n",
    "    print(f\"x: {data[0].size()}\")\n",
    "    print(f\"y: {data[1].size()}\")\n",
    "    batch_X, batch_Y = data\n",
    "    batch_X = batch_X.to(DEVICE)\n",
    "    batch_Y = batch_Y.to(DEVICE)\n",
    "    \n",
    "    batch_X.requires_grad_(True)\n",
    "    embeddings = backbone.encode_image(batch_X)\n",
    "    projs = posthoc_layer.compute_dist(embeddings)\n",
    "    predicted_Y = posthoc_layer.forward_projs(projs)\n",
    "    accuracy = (predicted_Y.argmax(1) == batch_Y).float().mean().item()\n",
    "    \n",
    "    _, topk_indices = torch.topk(projs, 5, dim=1)\n",
    "    topk_concept = [[posthoc_layer.names[idx] for idx in row] for row in topk_indices]\n",
    "\n",
    "    \n",
    "    show_image(batch_X.detach().cpu())\n",
    "    print(f\"embeddings: {embeddings.size()}\")\n",
    "    print(f\"projections: {projs.size()}\")\n",
    "    print(f\"predicted_Y: {predicted_Y.size()}\")\n",
    "    print(f\"accuracy: {accuracy}\")\n",
    "    # accuracy_idx.append(accuracy)\n",
    "    import pdb; pdb.set_trace()\n",
    "    \n",
    "# print(accuracy_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import os\n",
    "from constants import dataset_cosntants\n",
    "from pcbm.data.cub import CUBConceptDataset, get_concept_dicts\n",
    "from pcbm.concepts import ConceptBank\n",
    "\n",
    "CUB_CONCEPT_BANK_PATH =  \"/home/ksas/Public/datasets/cub_concept_bank/cub_resnet18_cub_0.1_100.pkl\"\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "TRAIN_PKL = os.path.join(dataset_cosntants.CUB_PROCESSED_DIR, \"train.pkl\")\n",
    "metadata = pkl.load(open(TRAIN_PKL, \"rb\"))\n",
    "\n",
    "concept_info = get_concept_dicts(metadata=metadata)\n",
    "concept_info[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(concept_info.__len__())\n",
    "print(concept_info[0][0].__len__())\n",
    "print(concept_info[0][1].__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Layer Grad CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "from clip.model import CLIP, ModifiedResNet, VisionTransformer\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "BACKBONE_NAME = \"clip:ViT-B/32\"\n",
    "DEVICE = \"cuda\"\n",
    "clip_backbone_name = BACKBONE_NAME.split(\":\")[1]\n",
    "backbone, preprocess = clip.load(clip_backbone_name, device=DEVICE, download_root=\"/home/ksas/Public/model_zoo/clip\")\n",
    "backbone = backbone.eval()\n",
    "backbone = backbone.float()\n",
    "normalizer = transforms.Compose(preprocess.transforms[-1:])\n",
    "preprocess = transforms.Compose(preprocess.transforms[:-1])\n",
    "print(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from explain_utils import *\n",
    "from captum.attr import visualization, GradientAttribution, LayerAttribution\n",
    "\n",
    "image_attn_blocks = list(dict(backbone.visual.transformer.resblocks.named_children()).values())\n",
    "last_blocks = image_attn_blocks[-1].ln_1\n",
    "layer_grad_cam = layer_grad_cam_vit(backbone,\n",
    "                                last_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_utils import *\n",
    "\n",
    "image:torch.Tensor = preprocess(Image.open(\"data/images/cat_and_dog.jpg\")).unsqueeze(0).to(DEVICE)\n",
    "text = clip.tokenize([\"car\", \"a dog\", \"a cat\"]).to(DEVICE)\n",
    "print(backbone(normalizer(image), text))\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 1, additional_args = {\"text\": text})\n",
    "print(attributions)\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=False,\n",
    "        save_to=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image:torch.Tensor = preprocess(Image.open(\"data/images/multi_dog_and_cat.jpg\")).unsqueeze(0).to(DEVICE)\n",
    "text = clip.tokenize([\"cat and dog\", \"dog\", \"cat\", \"many dogs\", \"many cats\"]).to(DEVICE)\n",
    "print(backbone(normalizer(image), text))\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 2, additional_args = {\"text\": text})\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=False,\n",
    "        save_to=None)\n",
    "\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 1, additional_args = {\"text\": text})\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=False,\n",
    "        save_to=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image:torch.Tensor = preprocess(Image.open(\"data/layer_grad_cam/propellers_images/946-original_image.jpg\")).unsqueeze(0).to(DEVICE)\n",
    "text = clip.tokenize([\"airplane\", \"propellers\", \"landing gear\"]).to(DEVICE)\n",
    "print(backbone(normalizer(image), text))\n",
    "\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 0, additional_args = {\"text\": text})\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=True,\n",
    "        save_to=None)\n",
    "\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 1, additional_args = {\"text\": text})\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=True,\n",
    "        save_to=None)\n",
    "\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 2, additional_args = {\"text\": text})\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=True,\n",
    "        save_to=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image:torch.Tensor = preprocess(Image.open(\"data/images/dog_cat.jpg\")).unsqueeze(0).to(DEVICE)\n",
    "text = clip.tokenize([\"car\", \"dog\", \"cat\"]).to(DEVICE)\n",
    "print(backbone(normalizer(image), text))\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 2, additional_args = {\"text\": text})\n",
    "print(attributions)\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=False,\n",
    "        save_to=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image:torch.Tensor = preprocess(Image.open(\"data/images/glasses.png\")).unsqueeze(0).to(DEVICE)\n",
    "text = clip.tokenize([\"man with eyeglasses\"]).to(DEVICE)\n",
    "print(backbone(normalizer(image), text))\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 0, additional_args = {\"text\": text})\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=True,\n",
    "        save_to=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test OPEN_CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RN50', 'openai'),\n",
       " ('RN50', 'yfcc15m'),\n",
       " ('RN50', 'cc12m'),\n",
       " ('RN50-quickgelu', 'openai'),\n",
       " ('RN50-quickgelu', 'yfcc15m'),\n",
       " ('RN50-quickgelu', 'cc12m'),\n",
       " ('RN101', 'openai'),\n",
       " ('RN101', 'yfcc15m'),\n",
       " ('RN101-quickgelu', 'openai'),\n",
       " ('RN101-quickgelu', 'yfcc15m'),\n",
       " ('RN50x4', 'openai'),\n",
       " ('RN50x16', 'openai'),\n",
       " ('RN50x64', 'openai'),\n",
       " ('ViT-B-32', 'openai'),\n",
       " ('ViT-B-32', 'laion400m_e31'),\n",
       " ('ViT-B-32', 'laion400m_e32'),\n",
       " ('ViT-B-32', 'laion2b_e16'),\n",
       " ('ViT-B-32', 'laion2b_s34b_b79k'),\n",
       " ('ViT-B-32', 'datacomp_xl_s13b_b90k'),\n",
       " ('ViT-B-32', 'datacomp_m_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_clip_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_laion_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_image_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_text_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_basic_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_s128m_b4k'),\n",
       " ('ViT-B-32', 'datacomp_s_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_clip_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_laion_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_image_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_text_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_basic_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_s13m_b4k'),\n",
       " ('ViT-B-32-256', 'datacomp_s34b_b86k'),\n",
       " ('ViT-B-32-quickgelu', 'openai'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e31'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e32'),\n",
       " ('ViT-B-32-quickgelu', 'metaclip_400m'),\n",
       " ('ViT-B-32-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-B-16', 'openai'),\n",
       " ('ViT-B-16', 'laion400m_e31'),\n",
       " ('ViT-B-16', 'laion400m_e32'),\n",
       " ('ViT-B-16', 'laion2b_s34b_b88k'),\n",
       " ('ViT-B-16', 'datacomp_xl_s13b_b90k'),\n",
       " ('ViT-B-16', 'datacomp_l_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_clip_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_laion_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_image_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_text_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_basic_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_s1b_b8k'),\n",
       " ('ViT-B-16', 'dfn2b'),\n",
       " ('ViT-B-16-quickgelu', 'metaclip_400m'),\n",
       " ('ViT-B-16-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e31'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'openai'),\n",
       " ('ViT-L-14', 'laion400m_e31'),\n",
       " ('ViT-L-14', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'laion2b_s32b_b82k'),\n",
       " ('ViT-L-14', 'datacomp_xl_s13b_b90k'),\n",
       " ('ViT-L-14', 'commonpool_xl_clip_s13b_b90k'),\n",
       " ('ViT-L-14', 'commonpool_xl_laion_s13b_b90k'),\n",
       " ('ViT-L-14', 'commonpool_xl_s13b_b90k'),\n",
       " ('ViT-L-14-quickgelu', 'metaclip_400m'),\n",
       " ('ViT-L-14-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-L-14-quickgelu', 'dfn2b'),\n",
       " ('ViT-L-14-336', 'openai'),\n",
       " ('ViT-H-14', 'laion2b_s32b_b79k'),\n",
       " ('ViT-H-14-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-H-14-quickgelu', 'dfn5b'),\n",
       " ('ViT-H-14-378-quickgelu', 'dfn5b'),\n",
       " ('ViT-g-14', 'laion2b_s12b_b42k'),\n",
       " ('ViT-g-14', 'laion2b_s34b_b88k'),\n",
       " ('ViT-bigG-14', 'laion2b_s39b_b160k'),\n",
       " ('ViT-bigG-14-quickgelu', 'metaclip_fullcc'),\n",
       " ('roberta-ViT-B-32', 'laion2b_s12b_b32k'),\n",
       " ('xlm-roberta-base-ViT-B-32', 'laion5b_s13b_b90k'),\n",
       " ('xlm-roberta-large-ViT-H-14', 'frozen_laion5b_s13b_b90k'),\n",
       " ('convnext_base', 'laion400m_s13b_b51k'),\n",
       " ('convnext_base_w', 'laion2b_s13b_b82k'),\n",
       " ('convnext_base_w', 'laion2b_s13b_b82k_augreg'),\n",
       " ('convnext_base_w', 'laion_aesthetic_s13b_b82k'),\n",
       " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k'),\n",
       " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k_augreg'),\n",
       " ('convnext_large_d', 'laion2b_s26b_b102k_augreg'),\n",
       " ('convnext_large_d_320', 'laion2b_s29b_b131k_ft'),\n",
       " ('convnext_large_d_320', 'laion2b_s29b_b131k_ft_soup'),\n",
       " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg'),\n",
       " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg_rewind'),\n",
       " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg_soup'),\n",
       " ('coca_ViT-B-32', 'laion2b_s13b_b90k'),\n",
       " ('coca_ViT-B-32', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
       " ('coca_ViT-L-14', 'laion2b_s13b_b90k'),\n",
       " ('coca_ViT-L-14', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
       " ('EVA01-g-14', 'laion400m_s11b_b41k'),\n",
       " ('EVA01-g-14-plus', 'merged2b_s11b_b114k'),\n",
       " ('EVA02-B-16', 'merged2b_s8b_b131k'),\n",
       " ('EVA02-L-14', 'merged2b_s4b_b131k'),\n",
       " ('EVA02-L-14-336', 'merged2b_s6b_b61k'),\n",
       " ('EVA02-E-14', 'laion2b_s4b_b115k'),\n",
       " ('EVA02-E-14-plus', 'laion2b_s9b_b144k'),\n",
       " ('ViT-B-16-SigLIP', 'webli'),\n",
       " ('ViT-B-16-SigLIP-256', 'webli'),\n",
       " ('ViT-B-16-SigLIP-i18n-256', 'webli'),\n",
       " ('ViT-B-16-SigLIP-384', 'webli'),\n",
       " ('ViT-B-16-SigLIP-512', 'webli'),\n",
       " ('ViT-L-16-SigLIP-256', 'webli'),\n",
       " ('ViT-L-16-SigLIP-384', 'webli'),\n",
       " ('ViT-SO400M-14-SigLIP', 'webli'),\n",
       " ('ViT-SO400M-16-SigLIP-i18n-256', 'webli'),\n",
       " ('ViT-SO400M-14-SigLIP-378', 'webli'),\n",
       " ('ViT-SO400M-14-SigLIP-384', 'webli'),\n",
       " ('ViT-L-14-CLIPA', 'datacomp1b'),\n",
       " ('ViT-L-14-CLIPA-336', 'datacomp1b'),\n",
       " ('ViT-H-14-CLIPA', 'datacomp1b'),\n",
       " ('ViT-H-14-CLIPA-336', 'laion2b'),\n",
       " ('ViT-H-14-CLIPA-336', 'datacomp1b'),\n",
       " ('ViT-bigG-14-CLIPA', 'datacomp1b'),\n",
       " ('ViT-bigG-14-CLIPA-336', 'datacomp1b'),\n",
       " ('nllb-clip-base', 'v1'),\n",
       " ('nllb-clip-large', 'v1'),\n",
       " ('nllb-clip-base-siglip', 'v1'),\n",
       " ('nllb-clip-base-siglip', 'mrl'),\n",
       " ('nllb-clip-large-siglip', 'v1'),\n",
       " ('nllb-clip-large-siglip', 'mrl'),\n",
       " ('MobileCLIP-S1', 'datacompdr'),\n",
       " ('MobileCLIP-S2', 'datacompdr'),\n",
       " ('MobileCLIP-B', 'datacompdr'),\n",
       " ('MobileCLIP-B', 'datacompdr_lt'),\n",
       " ('ViTamin-S', 'datacomp1b'),\n",
       " ('ViTamin-S-LTT', 'datacomp1b'),\n",
       " ('ViTamin-B', 'datacomp1b'),\n",
       " ('ViTamin-B-LTT', 'datacomp1b'),\n",
       " ('ViTamin-L', 'datacomp1b'),\n",
       " ('ViTamin-L-256', 'datacomp1b'),\n",
       " ('ViTamin-L-336', 'datacomp1b'),\n",
       " ('ViTamin-L-384', 'datacomp1b'),\n",
       " ('ViTamin-L2', 'datacomp1b'),\n",
       " ('ViTamin-L2-256', 'datacomp1b'),\n",
       " ('ViTamin-L2-336', 'datacomp1b'),\n",
       " ('ViTamin-L2-384', 'datacomp1b'),\n",
       " ('ViTamin-XL-256', 'datacomp1b'),\n",
       " ('ViTamin-XL-336', 'datacomp1b'),\n",
       " ('ViTamin-XL-384', 'datacomp1b')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import open_clip\n",
    "open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    <function _convert_to_rgb at 0x797c2cbf7240>\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
      ")\n",
      "Label probs: tensor([[0.1177, 0.5364, 0.3459]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai', cache_dir=\"/home/ksas/Public/model_zoo/clip\")\n",
    "model.eval()  # model in train mode by default, impacts some models with BatchNorm or stochastic depth active\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "\n",
    "image = preprocess(Image.open(\"data/images/cat_and_dog.jpg\")).unsqueeze(0)\n",
    "text = tokenizer([\"a diagram\", \"a dog\", \"a cat\"])\n",
    "print(preprocess)\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Label probs:\", text_probs) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pcbm_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
