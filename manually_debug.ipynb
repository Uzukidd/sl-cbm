{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from pcbm.data import get_dataset\n",
    "from pcbm.concepts import ConceptBank\n",
    "from pcbm.models import PosthocLinearCBM, get_model\n",
    "from pcbm.training_tools import load_or_compute_projections\n",
    "\n",
    "UNIVERSAL_SEED = 2024\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 4\n",
    "CONCEPT_BANK_PATH = \"/home/ksas/Public/datasets/cifar10_concept_bank/multimodal_concept_clip:RN50_cifar10_recurse:1.pkl\"\n",
    "OUT_PUT_DIR_PATH = \"exps/test\"\n",
    "CKPT_PATH = \"data/ckpt/CIFAR_10/pcbm_cifar10__clip:RN50__multimodal_concept_clip:RN50_cifar10_recurse:1__lam:0.0002__alpha:0.99__seed:42.ckpt\"\n",
    "DATASET_PATH = \"/home/ksas/Public/datasets/cifar10_concept_bank\"\n",
    "BACKBONE_NAME = \"clip:ViT-B/32\"\n",
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "set_random_seed(UNIVERSAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_concepts = pkl.load(open(CONCEPT_BANK_PATH, 'rb'))\n",
    "all_concept_names = list(all_concepts.keys())\n",
    "print(f\"Bank path: {CONCEPT_BANK_PATH}. {len(all_concept_names)} concepts will be used.\")\n",
    "concept_bank = ConceptBank(all_concepts, DEVICE)\n",
    "\n",
    "import clip\n",
    "clip_backbone_name = BACKBONE_NAME.split(\":\")[1]\n",
    "backbone, preprocess = clip.load(clip_backbone_name, device=DEVICE, download_root=\"/home/ksas/Public/model_zoo/clip\")\n",
    "backbone = backbone.eval()\n",
    "backbone = backbone.float()\n",
    "model = None\n",
    "\n",
    "backbone = backbone.to(DEVICE)\n",
    "backbone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cifar10_concept.txt\", \"w+\") as input_stream:\n",
    "    for idx, concept_name in enumerate(concept_bank.concept_info.concept_names):\n",
    "        input_stream.write(f\"{idx}\\t-{concept_name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posthoc_layer:PosthocLinearCBM = torch.load(CKPT_PATH, map_location=DEVICE)\n",
    "print(posthoc_layer.analyze_classifier(k=5))\n",
    "print(posthoc_layer.names)\n",
    "print(posthoc_layer.names.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from pcbm.learn_concepts_multimodal import *\n",
    "trainset = datasets.CIFAR10(root=DATASET_PATH, train=True,\n",
    "                            download=True, transform=preprocess)\n",
    "testset = datasets.CIFAR10(root=DATASET_PATH, train=False,\n",
    "                            download=True, transform=preprocess)\n",
    "classes = trainset.classes\n",
    "class_to_idx = {c: i for (i,c) in enumerate(classes)}\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "                                    shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(images:torch.Tensor):\n",
    "    import torch\n",
    "    import torchvision\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # 使用 torchvision.utils.make_grid 将 64 张图片排列成 8x8 的网格\n",
    "    grid_img = torchvision.utils.make_grid(images, nrow=8, normalize=True)\n",
    "\n",
    "    # 转换为 NumPy 格式以便用 matplotlib 显示\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))  # 转换为 [H, W, C]\n",
    "    plt.axis('off')  # 隐藏坐标轴\n",
    "    plt.show()\n",
    "\n",
    "for idx, data in enumerate(train_loader):\n",
    "    print(data.__len__())\n",
    "    print(f\"x: {data[0].size()}\")\n",
    "    print(f\"y: {data[1].size()}\")\n",
    "    batch_X, batch_Y = data\n",
    "    batch_X = batch_X.to(DEVICE)\n",
    "    batch_Y = batch_Y.to(DEVICE)\n",
    "    \n",
    "    batch_X.requires_grad_(True)\n",
    "    embeddings = backbone.encode_image(batch_X)\n",
    "    projs = posthoc_layer.compute_dist(embeddings)\n",
    "    predicted_Y = posthoc_layer.forward_projs(projs)\n",
    "    accuracy = (predicted_Y.argmax(1) == batch_Y).float().mean().item()\n",
    "    \n",
    "    _, topk_indices = torch.topk(projs, 5, dim=1)\n",
    "    topk_concept = [[posthoc_layer.names[idx] for idx in row] for row in topk_indices]\n",
    "\n",
    "    \n",
    "    show_image(batch_X.detach().cpu())\n",
    "    print(f\"embeddings: {embeddings.size()}\")\n",
    "    print(f\"projections: {projs.size()}\")\n",
    "    print(f\"predicted_Y: {predicted_Y.size()}\")\n",
    "    print(f\"accuracy: {accuracy}\")\n",
    "    # accuracy_idx.append(accuracy)\n",
    "    import pdb; pdb.set_trace()\n",
    "    \n",
    "# print(accuracy_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import os\n",
    "from constants import dataset_cosntants\n",
    "from pcbm.data.cub import CUBConceptDataset, get_concept_dicts\n",
    "from pcbm.concepts import ConceptBank\n",
    "\n",
    "CUB_CONCEPT_BANK_PATH =  \"/home/ksas/Public/datasets/cub_concept_bank/cub_resnet18_cub_0.1_100.pkl\"\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "TRAIN_PKL = os.path.join(dataset_cosntants.CUB_PROCESSED_DIR, \"train.pkl\")\n",
    "metadata = pkl.load(open(TRAIN_PKL, \"rb\"))\n",
    "\n",
    "concept_info = get_concept_dicts(metadata=metadata)\n",
    "concept_info[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(concept_info.__len__())\n",
    "print(concept_info[0][0].__len__())\n",
    "print(concept_info[0][1].__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Layer Grad CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "from clip.model import CLIP, ModifiedResNet, VisionTransformer\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "BACKBONE_NAME = \"clip:ViT-B/32\"\n",
    "DEVICE = \"cuda\"\n",
    "clip_backbone_name = BACKBONE_NAME.split(\":\")[1]\n",
    "backbone, preprocess = clip.load(clip_backbone_name, device=DEVICE, download_root=\"/home/ksas/Public/model_zoo/clip\")\n",
    "backbone = backbone.eval()\n",
    "backbone = backbone.float()\n",
    "normalizer = transforms.Compose(preprocess.transforms[-1:])\n",
    "preprocess = transforms.Compose(preprocess.transforms[:-1])\n",
    "print(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.explain_utils import *\n",
    "from captum.attr import visualization, GradientAttribution, LayerAttribution\n",
    "\n",
    "image_attn_blocks = list(dict(backbone.visual.transformer.resblocks.named_children()).values())\n",
    "last_blocks = image_attn_blocks[-1].ln_1\n",
    "layer_grad_cam = layer_grad_cam_vit(backbone,\n",
    "                                last_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual_utils import *\n",
    "\n",
    "image:torch.Tensor = preprocess(Image.open(\"data/images/cat_and_dog.jpg\")).unsqueeze(0).to(DEVICE)\n",
    "text = clip.tokenize([\"car\", \"a dog\", \"a cat\"]).to(DEVICE)\n",
    "print(backbone(normalizer(image), text))\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 1, additional_args = {\"text\": text})\n",
    "print(attributions)\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=False,\n",
    "        save_to=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image:torch.Tensor = preprocess(Image.open(\"data/images/multi_dog_and_cat.jpg\")).unsqueeze(0).to(DEVICE)\n",
    "text = clip.tokenize([\"cat and dog\", \"dog\", \"cat\", \"many dogs\", \"many cats\", \"eyees\"]).to(DEVICE)\n",
    "print(backbone(normalizer(image), text))\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 2, additional_args = {\"text\": text})\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=False,\n",
    "        save_to=None)\n",
    "\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 5, additional_args = {\"text\": text})\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=False,\n",
    "        save_to=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image:torch.Tensor = preprocess(Image.open(\"data/layer_grad_cam/propellers_images/946-original_image.jpg\")).unsqueeze(0).to(DEVICE)\n",
    "text = clip.tokenize([\"airplane\", \"propellers\", \"landing gear\"]).to(DEVICE)\n",
    "print(backbone(normalizer(image), text))\n",
    "\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 0, additional_args = {\"text\": text})\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=True,\n",
    "        save_to=None)\n",
    "\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 1, additional_args = {\"text\": text})\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=True,\n",
    "        save_to=None)\n",
    "\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 2, additional_args = {\"text\": text})\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=True,\n",
    "        save_to=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image:torch.Tensor = preprocess(Image.open(\"data/images/dog_cat.jpg\")).unsqueeze(0).to(DEVICE)\n",
    "text = clip.tokenize([\"car\", \"dog\", \"cat\"]).to(DEVICE)\n",
    "print(backbone(normalizer(image), text))\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 2, additional_args = {\"text\": text})\n",
    "print(attributions)\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=False,\n",
    "        save_to=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image:torch.Tensor = preprocess(Image.open(\"data/images/glasses.png\")).unsqueeze(0).to(DEVICE)\n",
    "text = clip.tokenize([\"man with eyeglasses\"]).to(DEVICE)\n",
    "print(backbone(normalizer(image), text))\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 0, additional_args = {\"text\": text})\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=True,\n",
    "        save_to=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image:torch.Tensor = preprocess(Image.open(\"data/images/airplane.jpg\")).unsqueeze(0).to(DEVICE)\n",
    "text = clip.tokenize([\"propellers\",\n",
    "                      \"heavier-than-air craft\",\n",
    "                      \"fuselage\",\n",
    "                      \"accelerator\",\n",
    "                      \"landing gear\"]).to(DEVICE)\n",
    "print(backbone(normalizer(image), text))\n",
    "attributions:torch.Tensor = layer_grad_cam.attribute(normalizer(image), 4, additional_args = {\"text\": text})\n",
    "upsampled_attr = LayerAttribution.interpolate(attributions, image.size()[-2:], interpolate_mode=\"bicubic\")\n",
    "\n",
    "viz_attn(image,\n",
    "        upsampled_attr,\n",
    "        blur=True,\n",
    "        save_to=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test OPEN_CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai', cache_dir=\"/home/ksas/Public/model_zoo/clip\")\n",
    "model.eval()  # model in train mode by default, impacts some models with BatchNorm or stochastic depth active\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "\n",
    "image = preprocess(Image.open(\"data/images/cat_and_dog.jpg\")).unsqueeze(0)\n",
    "text = tokenizer([\"a diagram\", \"a dog\", \"a cat\"])\n",
    "print(preprocess)\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Label probs:\", text_probs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from constants import dataset_constants\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "trainset = datasets.CIFAR10(root=dataset_constants.CIFAR10_DIR, train=True,\n",
    "                            download=False, transform=None)\n",
    "testset = datasets.CIFAR10(root=dataset_constants.CIFAR10_DIR, train=False,\n",
    "                            download=False, transform=None)\n",
    "classes = trainset.classes\n",
    "class_to_idx = {c: i for (i,c) in enumerate(classes)}\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(idx_to_class)\n",
    "plt.imshow(trainset[100][0])\n",
    "plt.axis('off')  # 隐藏坐标轴\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "CIFAR10_TRAINING_PATH = \"/home/ksas/Public/datasets/cifar10_concept_bank/training_imgs\"\n",
    "os.makedirs(CIFAR10_TRAINING_PATH, exist_ok=True)\n",
    "for idx, (image, cls) in tqdm(enumerate(trainset), total=trainset.__len__()):\n",
    "    image.save(os.path.join(CIFAR10_TRAINING_PATH, f\"{idx:05d}_{idx_to_class[cls]}.png\"), format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR10_TESTING_PATH = \"/home/ksas/Public/datasets/cifar10_concept_bank/testing_imgs\"\n",
    "os.makedirs(CIFAR10_TESTING_PATH, exist_ok=True)\n",
    "for idx, (image, cls) in tqdm(enumerate(testset), total=testset.__len__()):\n",
    "    image.save(os.path.join(CIFAR10_TESTING_PATH, f\"{idx:05d}_{idx_to_class[cls]}.png\"), format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 读取 CSV 文件\n",
    "df = pd.read_csv('/home/ksas/Public/datasets/cifar10_concept_bank/training_imgs.csv')\n",
    "\n",
    "# 打印前几行，验证读取是否成功\n",
    "print(df.head())\n",
    "print(df[\"filepath\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP full-shot evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_clip import get_input_dtype, get_tokenizer, build_zero_shot_classifier, \\\n",
    "    IMAGENET_CLASSNAMES, OPENAI_IMAGENET_TEMPLATES\n",
    "from open_clip_train.precision import get_autocast\n",
    "\n",
    "CIFAR10_CLASSNAME = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    pred = output.topk(max(topk), 1, True, True)[1].t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    return [float(correct[:k].reshape(-1).float().sum(0, keepdim=True).cpu().numpy()) for k in topk]\n",
    "\n",
    "\n",
    "def run(model, classifier, dataloader):\n",
    "    device = torch.device(\"cuda\")\n",
    "    autocast = get_autocast(\"amp\", device_type=device.type)\n",
    "    input_dtype = get_input_dtype(\"amp\")\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        top1, top5, n = 0., 0., 0.\n",
    "        for images, target in tqdm(dataloader, unit_scale=64):\n",
    "            images = images.to(device=device, dtype=input_dtype)\n",
    "            target = target.to(device)\n",
    "\n",
    "            with autocast():\n",
    "                # predict\n",
    "                output = model(image=images)\n",
    "                image_features = output['image_features'] if isinstance(output, dict) else output[0]\n",
    "                logits = 100. * image_features @ classifier\n",
    "\n",
    "            # measure accuracy\n",
    "            acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n",
    "            top1 += acc1\n",
    "            top5 += acc5\n",
    "            n += images.size(0)\n",
    "\n",
    "    top1 = (top1 / n)\n",
    "    top5 = (top5 / n)\n",
    "    return top1, top5\n",
    " \n",
    "def zero_shot_eval(model, dataloader, tokenizer):\n",
    "    print('Starting zero-shot imagenet.')\n",
    "    print('Building zero-shot classifier')\n",
    "    device = torch.device(\"cuda\")\n",
    "    autocast = get_autocast(\"amp\", device_type=device.type)\n",
    "    with autocast():\n",
    "        classifier = build_zero_shot_classifier(\n",
    "            model,\n",
    "            tokenizer=tokenizer,\n",
    "            classnames=CIFAR10_CLASSNAME,\n",
    "            templates= (\n",
    "                lambda c: f'a picture of a {c}.',\n",
    "            ),\n",
    "            num_classes_per_batch=10,\n",
    "            device=device,\n",
    "            use_tqdm=True,\n",
    "        )\n",
    "\n",
    "    print('Using classifier')\n",
    "    results = {}\n",
    "    top1, top5 = run(model, classifier, dataloader)\n",
    "    print(f\"top1: {top1}, top5: {top5}\")\n",
    "    print('Finished zero-shot imagenet.')\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "MODEL_NAME = 'ViT-B-32'\n",
    "ORIGINAL_CKPT_PATH = \"/home/ksas/Public/model_zoo/clip/ViT-B-32.pt\"\n",
    "REFINED_CKPT_PATH = \"/home/ksas/uzuki_space/open_clip/src/logs/2024_10_24-10_48_09-model_ViT-B-32-lr_1e-05-b_128-j_3-p_amp/checkpoints/epoch_2.pt\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(MODEL_NAME, pretrained=ORIGINAL_CKPT_PATH, cache_dir=\"/home/ksas/Public/model_zoo/clip\")\n",
    "model.cuda().eval()\n",
    "tokenizer = open_clip.get_tokenizer(MODEL_NAME)\n",
    "\n",
    "from common_utils import *\n",
    "\n",
    "trainset, testset, class_to_idx, idx_to_class, train_loader, test_loader = load_dataset(dataset_configure(\n",
    "            dataset = 'cifar10',\n",
    "            batch_size = 64,\n",
    "            num_workers = 4,\n",
    "        ), preprocess)\n",
    "\n",
    "zero_shot_eval(model, train_loader, tokenizer)\n",
    "zero_shot_eval(model, test_loader, tokenizer)\n",
    "\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(MODEL_NAME, pretrained=REFINED_CKPT_PATH, cache_dir=\"/home/ksas/Public/model_zoo/clip\")\n",
    "model.cuda().eval()\n",
    "\n",
    "zero_shot_eval(model, train_loader, tokenizer)\n",
    "zero_shot_eval(model, test_loader, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn import net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize CUB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bank path: /home/ksas/Public/datasets/cub_concept_bank/cub_resnet18_cub_0.1_100.pkl. 112 concepts will be used.\n",
      "Concept Bank is initialized.\n",
      "resnet18_cub\n",
      "ResNetBottom(\n",
      "  (features): Sequential(\n",
      "    (0): Sequential(\n",
      "      (init_block): ResInitBlock(\n",
      "        (conv): ConvBlock(\n",
      "          (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activ): ReLU(inplace=True)\n",
      "        )\n",
      "        (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (stage1): Sequential(\n",
      "        (unit1): ResUnit(\n",
      "          (body): ResBlock(\n",
      "            (conv1): ConvBlock(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (activ): ReLU(inplace=True)\n",
      "            )\n",
      "            (conv2): ConvBlock(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (activ): ReLU(inplace=True)\n",
      "        )\n",
      "        (unit2): ResUnit(\n",
      "          (body): ResBlock(\n",
      "            (conv1): ConvBlock(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (activ): ReLU(inplace=True)\n",
      "            )\n",
      "            (conv2): ConvBlock(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (activ): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (stage2): Sequential(\n",
      "        (unit1): ResUnit(\n",
      "          (body): ResBlock(\n",
      "            (conv1): ConvBlock(\n",
      "              (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (activ): ReLU(inplace=True)\n",
      "            )\n",
      "            (conv2): ConvBlock(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (identity_conv): ConvBlock(\n",
      "            (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activ): ReLU(inplace=True)\n",
      "        )\n",
      "        (unit2): ResUnit(\n",
      "          (body): ResBlock(\n",
      "            (conv1): ConvBlock(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (activ): ReLU(inplace=True)\n",
      "            )\n",
      "            (conv2): ConvBlock(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (activ): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (stage3): Sequential(\n",
      "        (unit1): ResUnit(\n",
      "          (body): ResBlock(\n",
      "            (conv1): ConvBlock(\n",
      "              (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (activ): ReLU(inplace=True)\n",
      "            )\n",
      "            (conv2): ConvBlock(\n",
      "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (identity_conv): ConvBlock(\n",
      "            (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activ): ReLU(inplace=True)\n",
      "        )\n",
      "        (unit2): ResUnit(\n",
      "          (body): ResBlock(\n",
      "            (conv1): ConvBlock(\n",
      "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (activ): ReLU(inplace=True)\n",
      "            )\n",
      "            (conv2): ConvBlock(\n",
      "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (activ): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (stage4): Sequential(\n",
      "        (unit1): ResUnit(\n",
      "          (body): ResBlock(\n",
      "            (conv1): ConvBlock(\n",
      "              (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (activ): ReLU(inplace=True)\n",
      "            )\n",
      "            (conv2): ConvBlock(\n",
      "              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (identity_conv): ConvBlock(\n",
      "            (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (activ): ReLU(inplace=True)\n",
      "        )\n",
      "        (unit2): ResUnit(\n",
      "          (body): ResBlock(\n",
      "            (conv1): ConvBlock(\n",
      "              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (activ): ReLU(inplace=True)\n",
      "            )\n",
      "            (conv2): ConvBlock(\n",
      "              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (activ): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (final_pool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "cub_mean_pxs = np.array([0.5, 0.5, 0.5])\n",
    "cub_std_pxs = np.array([2., 2., 2.])\n",
    "preprocess = transforms.Compose([\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(cub_mean_pxs, cub_std_pxs)\n",
    "        ])\n",
    "\n",
    "concept_bank = load_concept_bank(concept_bank_configure(\n",
    "            concept_bank = \"/home/ksas/Public/datasets/cub_concept_bank/cub_resnet18_cub_0.1_100.pkl\",\n",
    "            device = torch.device(\"cuda\")\n",
    "        ))\n",
    "backbone, preprocess = load_backbone(backbone_configure(\n",
    "    backbone_name=\"resnet18_cub\",\n",
    "    backbone_ckpt = \"/home/ksas/Public/model_zoo/resnet_cub\",\n",
    "    device = torch.device(\"cuda\")\n",
    "), full_load=False)\n",
    "normalizer = transforms.Compose(preprocess.transforms[-1:])\n",
    "preprocess = transforms.Compose(preprocess.transforms[:-1])\n",
    "\n",
    "posthoc_layer = load_pcbm(pcbm_configure(\n",
    "        pcbm_ckpt = \"data/ckpt/CUB/vanilla_pcbm_cub__resnet18_cub__cub_resnet18_cub_0__lam:0.0002__alpha:0.99__seed:42.ckpt\",\n",
    "        device = torch.device(\"cuda\")\n",
    "))\n",
    "\n",
    "# trainset, testset, class_to_idx, idx_to_class, train_loader, test_loader= load_dataset(dataset_configure(\n",
    "#         dataset = \"cub\",\n",
    "#         batch_size = 8,\n",
    "#         num_workers = 4,\n",
    "# ), preprocess)\n",
    "print(backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
     ]
    }
   ],
   "source": [
    "print(backbone.get_submodule(\"features\")\n",
    "      .get_submodule(\"0\")\n",
    "      .get_submodule(\"stage4\")\n",
    "      .get_submodule(\"unit2\")\n",
    "      .get_submodule(\"body\")\n",
    "      .get_submodule(\"conv2\")\n",
    "      .get_submodule(\"conv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils.visual_utils import show_image\n",
    "\n",
    "totall_accuracy = []\n",
    "for idx, data in enumerate(tqdm(train_loader)):\n",
    "    batch_X, batch_Y = data\n",
    "    # show_image(batch_X)\n",
    "    batch_X:torch.Tensor = batch_X.to(torch.device(\"cuda\"))\n",
    "    batch_Y:torch.Tensor = batch_Y.to(torch.device(\"cuda\"))\n",
    "    totall_accuracy.append((backbone(batch_X).argmax(1) == batch_Y).float().mean().item())\n",
    "    # concept = posthoc_layer.compute_dist(backbone.encode_image(batch_X))\n",
    "    \n",
    "    # import pdb; pdb.set_trace()\n",
    "    \n",
    "print(np.array(totall_accuracy).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize CUB concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from pcbm.data.constants import CUB_PROCESSED_DIR\n",
    "from pcbm.data.cub import CUBConceptDataset, get_concept_dicts\n",
    "\n",
    "TRAIN_PKL = os.path.join(CUB_PROCESSED_DIR, \"train.pkl\")\n",
    "metadata = pickle.load(open(TRAIN_PKL, \"rb\"))\n",
    "\n",
    "concept_info = get_concept_dicts(metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metadata[0].keys())\n",
    "print(metadata[0][\"attribute_certainty\"])\n",
    "\n",
    "img_1 = metadata[800]\n",
    "img_2 = None\n",
    "print(img_1[\"attribute_label\"].__len__())\n",
    "for img in metadata[1:]:\n",
    "    if img_1[\"class_label\"] == img[\"class_label\"]:\n",
    "        print(img_1[\"attribute_label\"] == img[\"attribute_label\"])\n",
    "        print(img_1[\"attribute_certainty\"] == img[\"attribute_certainty\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test casual metirc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bank path: /home/ksas/Public/datasets/cifar10_concept_bank/multimodal_concept_clip:RN50_cifar10_recurse:1.pkl. 175 concepts will be used.\n",
      "Concept Bank is initialized.\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import random\n",
    "import clip.model\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Callable, Union, Dict\n",
    "\n",
    "import clip\n",
    "from clip.model import CLIP, ModifiedResNet, VisionTransformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from pcbm.learn_concepts_multimodal import *\n",
    "from pcbm.data import get_dataset\n",
    "from pcbm.concepts import ConceptBank\n",
    "from pcbm.models import PosthocLinearCBM, get_model\n",
    "\n",
    "from captum.attr import visualization, GradientAttribution, LayerAttribution\n",
    "from utils import *\n",
    "\n",
    "target_classs = 3 # cat\n",
    "concept_target = \"sharp claws\"\n",
    "batch_size = 1\n",
    "explain_method = \"saliency_map\"\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "concept_bank = load_concept_bank(concept_bank_configure(\n",
    "    concept_bank = \"/home/ksas/Public/datasets/cifar10_concept_bank/multimodal_concept_clip:RN50_cifar10_recurse:1.pkl\",\n",
    "    device = device\n",
    "))\n",
    "backbone, preprocess = load_backbone(backbone_configure(\n",
    "    backbone_name = \"open_clip:RN50\",\n",
    "    backbone_ckpt = \"outputs/clip_adversarial_saliency_guided_training/adversarial_saliency_guided_training-open_clip:RN50.pth\",\n",
    "    # backbone_ckpt = \"/home/ksas/Public/model_zoo/clip\",\n",
    "    device = device\n",
    "    \n",
    "))\n",
    "normalizer = transforms.Compose(preprocess.transforms[-1:])\n",
    "preprocess = transforms.Compose(preprocess.transforms[:-1])\n",
    "\n",
    "posthoc_layer = load_pcbm(pcbm_configure(\n",
    "    pcbm_ckpt = \"data/ckpt/CIFAR_10/pcbm_cifar10__clip:RN50__multimodal_concept_clip:RN50_cifar10_recurse:1__lam:0.0002__alpha:0.99__seed:42.ckpt\",\n",
    "    device = device\n",
    "))\n",
    "dataset = load_dataset(dataset_configure(\n",
    "    dataset = \"cifar10\",\n",
    "    batch_size = batch_size,\n",
    "    num_workers = 4\n",
    "    ), preprocess, target_classs)\n",
    "    \n",
    "model_context = model_pipeline(concept_bank = concept_bank, \n",
    "                posthoc_layer = posthoc_layer, \n",
    "                preprocess = preprocess, \n",
    "                normalizer = normalizer, \n",
    "                backbone = backbone)\n",
    "posthoc_concept_net = PCBM_Net(model_context=model_context)\n",
    "\n",
    "explain_algorithm:GradientAttribution = getattr(model_explain_algorithm_factory, explain_method)(posthoc_concept_net = posthoc_concept_net)\n",
    "explain_algorithm_forward:Callable = getattr(model_explain_algorithm_forward, explain_method)\n",
    "targeted_concept_idx = model_context.concept_bank.concept_names.index(concept_target)\n",
    "print(targeted_concept_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([225, 1])\n",
      "tensor([[0.6759],\n",
      "        [0.6443],\n",
      "        [0.6391],\n",
      "        [0.6343],\n",
      "        [0.6298],\n",
      "        [0.6203],\n",
      "        [0.6150],\n",
      "        [0.6130],\n",
      "        [0.6074],\n",
      "        [0.6000],\n",
      "        [0.5902],\n",
      "        [0.5793],\n",
      "        [0.5734],\n",
      "        [0.5690],\n",
      "        [0.5676],\n",
      "        [0.5630],\n",
      "        [0.5607],\n",
      "        [0.5590],\n",
      "        [0.5582],\n",
      "        [0.5573],\n",
      "        [0.5550],\n",
      "        [0.5539],\n",
      "        [0.5548],\n",
      "        [0.5523],\n",
      "        [0.5497],\n",
      "        [0.5490],\n",
      "        [0.5477],\n",
      "        [0.5441],\n",
      "        [0.5428],\n",
      "        [0.5419],\n",
      "        [0.5438],\n",
      "        [0.5443],\n",
      "        [0.5436],\n",
      "        [0.5423],\n",
      "        [0.5436],\n",
      "        [0.5439],\n",
      "        [0.5444],\n",
      "        [0.5504],\n",
      "        [0.5507],\n",
      "        [0.5505],\n",
      "        [0.5549],\n",
      "        [0.5574],\n",
      "        [0.5610],\n",
      "        [0.5671],\n",
      "        [0.5683],\n",
      "        [0.5708],\n",
      "        [0.5731],\n",
      "        [0.5759],\n",
      "        [0.5763],\n",
      "        [0.5725],\n",
      "        [0.5737],\n",
      "        [0.5725],\n",
      "        [0.5702],\n",
      "        [0.5690],\n",
      "        [0.5678],\n",
      "        [0.5644],\n",
      "        [0.5610],\n",
      "        [0.5575],\n",
      "        [0.5575],\n",
      "        [0.5583],\n",
      "        [0.5578],\n",
      "        [0.5579],\n",
      "        [0.5578],\n",
      "        [0.5570],\n",
      "        [0.5586],\n",
      "        [0.5588],\n",
      "        [0.5587],\n",
      "        [0.5572],\n",
      "        [0.5563],\n",
      "        [0.5565],\n",
      "        [0.5565],\n",
      "        [0.5556],\n",
      "        [0.5554],\n",
      "        [0.5558],\n",
      "        [0.5559],\n",
      "        [0.5559],\n",
      "        [0.5557],\n",
      "        [0.5559],\n",
      "        [0.5561],\n",
      "        [0.5564],\n",
      "        [0.5561],\n",
      "        [0.5560],\n",
      "        [0.5557],\n",
      "        [0.5547],\n",
      "        [0.5552],\n",
      "        [0.5554],\n",
      "        [0.5556],\n",
      "        [0.5555],\n",
      "        [0.5550],\n",
      "        [0.5550],\n",
      "        [0.5547],\n",
      "        [0.5546],\n",
      "        [0.5531],\n",
      "        [0.5519],\n",
      "        [0.5512],\n",
      "        [0.5509],\n",
      "        [0.5519],\n",
      "        [0.5519],\n",
      "        [0.5525],\n",
      "        [0.5519],\n",
      "        [0.5517],\n",
      "        [0.5520],\n",
      "        [0.5520],\n",
      "        [0.5522],\n",
      "        [0.5525],\n",
      "        [0.5525],\n",
      "        [0.5539],\n",
      "        [0.5547],\n",
      "        [0.5548],\n",
      "        [0.5533],\n",
      "        [0.5536],\n",
      "        [0.5537],\n",
      "        [0.5545],\n",
      "        [0.5545],\n",
      "        [0.5540],\n",
      "        [0.5549],\n",
      "        [0.5558],\n",
      "        [0.5561],\n",
      "        [0.5562],\n",
      "        [0.5566],\n",
      "        [0.5568],\n",
      "        [0.5550],\n",
      "        [0.5554],\n",
      "        [0.5552],\n",
      "        [0.5545],\n",
      "        [0.5548],\n",
      "        [0.5545],\n",
      "        [0.5545],\n",
      "        [0.5551],\n",
      "        [0.5568],\n",
      "        [0.5577],\n",
      "        [0.5579],\n",
      "        [0.5574],\n",
      "        [0.5580],\n",
      "        [0.5589],\n",
      "        [0.5589],\n",
      "        [0.5591],\n",
      "        [0.5604],\n",
      "        [0.5606],\n",
      "        [0.5603],\n",
      "        [0.5602],\n",
      "        [0.5599],\n",
      "        [0.5577],\n",
      "        [0.5549],\n",
      "        [0.5552],\n",
      "        [0.5558],\n",
      "        [0.5566],\n",
      "        [0.5556],\n",
      "        [0.5540],\n",
      "        [0.5529],\n",
      "        [0.5539],\n",
      "        [0.5539],\n",
      "        [0.5543],\n",
      "        [0.5534],\n",
      "        [0.5532],\n",
      "        [0.5506],\n",
      "        [0.5508],\n",
      "        [0.5495],\n",
      "        [0.5503],\n",
      "        [0.5509],\n",
      "        [0.5516],\n",
      "        [0.5536],\n",
      "        [0.5524],\n",
      "        [0.5534],\n",
      "        [0.5526],\n",
      "        [0.5528],\n",
      "        [0.5543],\n",
      "        [0.5520],\n",
      "        [0.5503],\n",
      "        [0.5488],\n",
      "        [0.5486],\n",
      "        [0.5488],\n",
      "        [0.5472],\n",
      "        [0.5459],\n",
      "        [0.5451],\n",
      "        [0.5436],\n",
      "        [0.5431],\n",
      "        [0.5422],\n",
      "        [0.5425],\n",
      "        [0.5431],\n",
      "        [0.5424],\n",
      "        [0.5411],\n",
      "        [0.5413],\n",
      "        [0.5420],\n",
      "        [0.5418],\n",
      "        [0.5417],\n",
      "        [0.5420],\n",
      "        [0.5420],\n",
      "        [0.5422],\n",
      "        [0.5424],\n",
      "        [0.5427],\n",
      "        [0.5435],\n",
      "        [0.5440],\n",
      "        [0.5444],\n",
      "        [0.5449],\n",
      "        [0.5450],\n",
      "        [0.5455],\n",
      "        [0.5462],\n",
      "        [0.5462],\n",
      "        [0.5465],\n",
      "        [0.5470],\n",
      "        [0.5473],\n",
      "        [0.5476],\n",
      "        [0.5477],\n",
      "        [0.5483],\n",
      "        [0.5488],\n",
      "        [0.5493],\n",
      "        [0.5500],\n",
      "        [0.5503],\n",
      "        [0.5504],\n",
      "        [0.5509],\n",
      "        [0.5513],\n",
      "        [0.5521],\n",
      "        [0.5516],\n",
      "        [0.5515],\n",
      "        [0.5509],\n",
      "        [0.5508],\n",
      "        [0.5514],\n",
      "        [0.5502],\n",
      "        [0.5483],\n",
      "        [0.5464],\n",
      "        [0.5450],\n",
      "        [0.5447],\n",
      "        [0.5449],\n",
      "        [0.5443]])\n",
      "> \u001b[0;32m/tmp/ipykernel_3938208/2632531489.py\u001b[0m(13)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m                             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcept_bank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcept_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m\u001b[0mtotall_causal_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31mfor idx, data in tqdm(enumerate(dataset.test_loader), \n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m                          \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m    \u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:49<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "klen = 11\n",
    "ksig = 5\n",
    "kern = gkern(klen, ksig)\n",
    "\n",
    "blur = lambda x: nn.functional.conv2d(x, kern, padding=klen//2)\n",
    "\n",
    "causal_metric = CausalMetric(model = posthoc_concept_net,\n",
    "                             mode = \"del\",\n",
    "                             step = 224,\n",
    "                             substrate_fn = torch.zeros_like,\n",
    "                             classes = model_context.concept_bank.concept_names.__len__())\n",
    "totall_causal_metric = []\n",
    "for idx, data in tqdm(enumerate(dataset.test_loader), \n",
    "                          total=dataset.test_loader.__len__()):\n",
    "    batch_X, batch_Y = data\n",
    "    batch_X:torch.Tensor = batch_X.to(device)\n",
    "    batch_Y:torch.Tensor = batch_Y.to(device)\n",
    "\n",
    "    batch_X.requires_grad_(True)\n",
    "    attributions:torch.Tensor = explain_algorithm_forward(batch_X=batch_X, \n",
    "                                                            explain_algorithm=explain_algorithm,\n",
    "                                                            target=targeted_concept_idx)\n",
    "    causal_curve = causal_metric.forward(batch_X, \n",
    "                                         attributions.mean(1), \n",
    "                                         batch_X.size(0), \n",
    "                                         targeted_concept_idx * torch.ones_like(batch_Y))\n",
    "    print(causal_curve.size())\n",
    "    print(causal_curve)\n",
    "    causal_auc = auc(causal_curve)\n",
    "    totall_causal_metric.append(causal_auc.mean().item())\n",
    "    # for i in range(batch_Y.size(0)):\n",
    "    #     captum_vis_attn(batch_X[i:i+1], \n",
    "    #                 attributions[i:i+1], \n",
    "    #                 title=f\"{dataset.idx_to_class[batch_Y[i].item()]}-attributions: {concept_target}\",\n",
    "    #                 save_to=None)\n",
    "    import pdb; pdb.set_trace()\n",
    "    \n",
    "print(f\"totall causal metric:{np.array(totall_causal_metric).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vanilla: totall causal metric:0.34300549793988466\n",
    "asgt: totall causal metric:0.3920287359505892\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pcbm_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
